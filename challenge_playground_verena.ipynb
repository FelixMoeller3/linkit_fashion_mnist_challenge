{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDb1ilq8WZ5r"
   },
   "source": [
    "# Fashion MNIST Data Science Challenge: Neural Networks and Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mi1HaLfefo0q"
   },
   "source": [
    "## **Intuition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T01BhSUXioKW"
   },
   "source": [
    "### Our goal\n",
    "Classify clothes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjKc-XPe9F5N"
   },
   "source": [
    "![alt text](https://miro.medium.com/max/608/1*3QpK4Vhw0BpbjwijIYFkfQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels\n",
    "\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "<li>0 T-shirt/top </li>\n",
    "<li>1 Trouser</li>\n",
    "<li>2 Pullover </li>\n",
    "<li>3 Dress </li>\n",
    "<li>4 Coat </li>\n",
    "<li>5 Sandal</li>\n",
    "<li>6 Shirt </li>\n",
    "<li>7 Sneaker </li>\n",
    "<li>8 Bag </li>\n",
    "<li>9 Ankle boot </li>\n",
    "--------------------------\n",
    "\n",
    "Each row is a separate image\n",
    "\n",
    "Column 1 is the class label.\n",
    "Remaining columns are pixel numbers (784 total).\n",
    "Each value is the darkness of the pixel (1 to 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1Y8vixb7aCw"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_60sCVqkfs2a"
   },
   "source": [
    "![](http://neuralnetworksanddeeplearning.com/images/tikz12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgNC8OyiiStg"
   },
   "source": [
    "### Artificial Neuron\n",
    "Inputs are weighted, summed up, a bias is added and the result is transformed by a nonlinear activation function. \n",
    "![alt text](https://www.researchgate.net/publication/320270458/figure/fig1/AS:551197154254848@1508427050805/Mathematical-model-of-artificial-neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHg9cuxIsl7Z"
   },
   "source": [
    "### Activation function\n",
    "Squashes the weighted inputs into $[0, 1]$ (i.e. bounds the outputs) and introduces non-linearity to the model, e.g.\n",
    "\n",
    "**Sigmoid function**:\n",
    "\\begin{eqnarray} \n",
    "  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}\n",
    "  \\end{eqnarray}\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkzA-x6pgTNn"
   },
   "source": [
    "### Forward pass\n",
    "Outputs are calculated in a **feed-forward manner**: activated weighted sums are propagated through the network to the output layer.\n",
    "\n",
    "![alt text](https://glassboxmedicine.files.wordpress.com/2019/01/slide2.jpg?w=616)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQOAfuBSza20"
   },
   "source": [
    "### Backward pass\n",
    "Errors are calculated from the outputs and **backpropagated**: the network improves its predictions (it learns) by updating the weights and biases in each backward pass from the last to the first hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RpybfDFvgcbh"
   },
   "source": [
    "#### Loss function\n",
    "Or cost function; Compares actual with desired outputs of the network and therefore measures the predictive performance of the network. \n",
    "$\\begin{eqnarray} C(w,b) \\equiv\n",
    "  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2.\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfF-eoP9g75d"
   },
   "source": [
    "#### Optimization\n",
    "Improve the networks performance by minimizing the cost w.r.t. the network's parameters w and b: \n",
    "$\\begin{equation}\n",
    "\\begin{aligned} \\min_{w,b} C(w,b) \n",
    "\\end{aligned}\n",
    "\\end{equation}$ <br>\n",
    "Compute partial derivatives $\\partial C / \\partial w$ and $\\partial C / \\partial b$ to identify the slope of the cost function w.r.t. w and b and use **gradient descent** step downwards by updating the parameters:\n",
    "\n",
    "![alt text](http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png)\n",
    "\\begin{eqnarray}\n",
    "  w_k & \\rightarrow & w_k' = w_k-\\eta \\frac{\\partial C}{\\partial w_k} \\\\\n",
    "  b_l & \\rightarrow & b_l' = b_l-\\eta \\frac{\\partial C}{\\partial b_l} \\\\\n",
    "\\end{eqnarray}\n",
    "\\begin{align}\n",
    "\\eta = \\text{learning rate or step size}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "![alt text](https://datascience-enthusiast.com/figures/kiank_sgd.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsIT5MprhGis"
   },
   "source": [
    "#### Backpropagation\n",
    "Gradient descent requires the computation of partial derivatives $\\partial C / \\partial w$ and $\\partial C / \\partial b$ of the cost function C with respect to any weight w or bias b in the network. By means of the **backpropagation algorithm** those derivatives are computed efficiently.\n",
    "[Read more](http://neuralnetworksanddeeplearning.com/chap2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iQeajjDXCiS"
   },
   "source": [
    "## **Fashion Mnist Classification**\n",
    "Let's build a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4071,
     "status": "ok",
     "timestamp": 1588762079160,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "xTS0YM-sOJyf",
    "outputId": "5e739605-4550-4146-a90e-bd809119b88e"
   },
   "outputs": [],
   "source": [
    "#basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z33CvoON9uoF"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Fptwh8hO62p"
   },
   "outputs": [],
   "source": [
    "#1. Get the file\n",
    "data_train = pd.read_csv('train.csv')\n",
    "data_validate = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13209,
     "status": "ok",
     "timestamp": 1588762088350,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "fhZAkdQzPHW3",
    "outputId": "7fe8ad39-1856-4147-ae82-41502c201450"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>150</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>169</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>216</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>86</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      6       0       0       0       0       0       0       0       1   \n",
       "1      2       0       0       0       0       0       0       1       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      3       0       0       0       0       0       0       0       0   \n",
       "4      1       0       0       0       0       0       0       0       0   \n",
       "5      2       0       0       0       0       0       0       0       0   \n",
       "6      8       0       0       0       0       0       0       0       0   \n",
       "7      4       0       0       0       0       0       0       0       0   \n",
       "8      0       0       0       0       0       0       1       3       0   \n",
       "9      7       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     123  ...         0         0         0         0       127       150   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...       169        43         0         0         0         0   \n",
       "3       0  ...       129        37         0         0         0         0   \n",
       "4       0  ...         2         0         0         0         0         0   \n",
       "5       0  ...         4         0         0       188       216       103   \n",
       "6       0  ...         0         0         0         0         0         0   \n",
       "7       0  ...         0         0         0        57        86        63   \n",
       "8       0  ...       103        73         0         0         2         0   \n",
       "9       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0        28         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "5         0         0         0         0  \n",
       "6         0         0         0         0  \n",
       "7         0         0         0         0  \n",
       "8         0         0         0         0  \n",
       "9         0         0         0         0  \n",
       "\n",
       "[10 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.Explore train data\n",
    "data_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>237</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>131</td>\n",
       "      <td>...</td>\n",
       "      <td>179</td>\n",
       "      <td>184</td>\n",
       "      <td>162</td>\n",
       "      <td>179</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>62</td>\n",
       "      <td>70</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0          0       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       2       0       0       0       0       0   \n",
       "2          0       0       0       0       0       0       0       0       0   \n",
       "3          0       0       0       0       0       0       3       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9995       0       0       0       0       0       0       0       0       0   \n",
       "9996       0       0       0       0       0       0       0       0       0   \n",
       "9997       0       0       0       0       0       2       4       3       0   \n",
       "9998       0       0       0       0       3       0       0       0      21   \n",
       "9999       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0           0  ...         5         0         0         0         0   \n",
       "1           0  ...       125        32         3         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...        70        38         1         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "9995        0  ...        52         0         0         0         0   \n",
       "9996      120  ...         0         0         0         1         4   \n",
       "9997        0  ...         2         0         0         0       145   \n",
       "9998      131  ...       179       184       162       179        42   \n",
       "9999        0  ...         0         0         0        89        62   \n",
       "\n",
       "      pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0            0         0         0         0         0  \n",
       "1            0         0         0         0         0  \n",
       "2            0         0         0         0         0  \n",
       "3            0         0         0         0         0  \n",
       "4            0         0         0         0         0  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "9995         0         0         0         0         0  \n",
       "9996         1         0         0         0         0  \n",
       "9997       237        76         0         0         0  \n",
       "9998         0         0         0         0         0  \n",
       "9999        70        57         0         0         0  \n",
       "\n",
       "[10000 rows x 784 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13190,
     "status": "ok",
     "timestamp": 1588762088352,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "1OYj8kjVPHYj",
    "outputId": "ec09bbef-b233-4de7-948f-dfe676c051a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explore test data\n",
    "print(data_train.shape)\n",
    "data_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.array(data_train, dtype = 'float32') # Damit Input Daten von Keras akzeptiert werden müssen wir sie in ein Array umwandeln \n",
    "data_validate = np.array(data_validate, dtype='float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(data_train[0,1:].reshape((28,28)))\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = data_train[:,1:]/255 #pixel data from 0-1 TODO -0.5?\n",
    "x_train = data_train[:,1:]  # Added normalization Layer\n",
    "y_train = data_train[:,0] #label data\n",
    "\n",
    "#data_submission = data_validate/255  # TODO -0.5?\n",
    "data_submission = data_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAD4CAYAAACE9dGgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAb30lEQVR4nO3df4xd5X3n8fd3rq9n8A+wHYMxthcCcdo6VWPQQGiSpkRoE4emNXS3VviDeltaowi0ILHqUrTasO0ioSrAtmoW1RQaR4WkSECxIhTiWqlSqoZiUy/YuCmGmNpmbGMMZszg+XHvd/+4Z8Qdz5znOTPn/jhn5vOyjnzvee4555kzM995znO+53nM3RERKauebldARCQPBTERKTUFMREpNQUxESk1BTERKbV5nTzYfOv1PhZ28pCFMLw6/DVXPwhvXzk9Eiz30dHpVqllrFIJf2BeevnIkmpw03q4mN7DkRM3B53hA0Z82PLs48tfXOjvnKxl+uzul4efc/cNeY6XV64gZmYbgD8FKsBfuvt9oc/3sZDP2LV5DllMFv6ZeeP2q4Pl5+8Op7ks/cdDwfKxt46mF9az/TDOVOW8peEPfGxJatGh61cGNz1zQfi8XPoH/xQ+9hz0gu/MvY8TJ2u88NzqTJ+trnx9ee4D5jTjy0kzqwDfAr4CrANuNLN1raqYiHSLU/N6piXGzNaY2Y/M7FUz22dmtyfr7zGzI2a2J1mua9rmD83sgJn91My+HDtGnpbYVcABd38jOfD3gI3Aqzn2KSJd5kCdliXBjwF3uvtLZrYY2G1mO5KyB939m80fThpCXwM+BVwE/J2ZfdLdUy8p8nTsrwKar3MOJ+smMLMtZrbLzHaNMpzjcCLSKfWM/2LcfcDdX0peDwL7mSJONNkIfM/dh939Z8ABGg2mVG2/O+nuW9293937q/S2+3AikpPjjHo90wIsH2+kJMuWtP2a2SXA5cALyarbzOxlM3vUzMY7VzM1jprluZw8Aqxper86WSciJeZALfvl5Al37499yMwWAU8Cd7j7+2b2EPDHyeH+GLgf+N2Z1DdPS+xFYK2ZfdzM5tO4jt2eY38iUhB1PNOShZlVaQSwx9z9KQB3P+buNXevAw/z0SXjtBtHM26JufuYmd0GPEcjxeJRd9830/11W+Xcc4Pln/r7wdSyDee9HNz2M73hVICf/lb4b8knquE0ib0j6ZfpP1f9MLjtZ568M1i+6z89ECz/lT//b8Hy/3XzX6eWXdn3VnDbSJoYx34r/Ik/OvTV1LIPvvB2ZO8RkbQaSjw6jAO1FtXfzAx4BNjv7g80rV/p7gPJ2xuAvcnr7cDjZvYAjY79tcA/h46RK0/M3Z8Fns2zDxEpnniXfWafA24CXjGzPcm6u2mkZK2nETMPArcAuPs+M3uCRpbDGHBr6M4kdDhjX0SKz/Hp9ImF9+X+PDBVszW18ePu9wL3Zj2GgpiITOAOoyW6GlYQE5GzGLUpG0/FpCAmIhM4UFdLTETKTC0xESmtRrKrgljhVJacFyzf8uLuYPn1C0+nlu0bCedi7R2ZHyzvsfAN7Z+Ohr9Ni3vSxxs7PBbetm9Nev4bwFu18A/z418P55FVAne5jtbCj6HVPZw/FztvT31iR2rZJ+/7enDbj98VGeanxHlgMQ6MRs59kcyZICYi2ThGrUSDPiuIicgkddflpIiUlPrERKTkjJr6xESkrBojuyqIiUhJuRsjHpnFqkDmTBAbeiI8K08ohQLgx2fSy1ZVwkPl1HrCU6qdyfkDE+qE7bFwKsC+X34sWP6TM+H0kDwWRM4LFj6vfZHyfSNjqWV/telbwW3/972/GiyvD4ZTU6waPm8+Gp6Gr9vq6hMTkbJqdOzrclJESksd+yJSYurYF5HSqynZVUTKyjFGvTyhoTw1FZGOUMe+iJSaY7qcLKKBd8NTsg3Vw3k75wbyrc7rCX/DB8fC5bF8p9iwKCOBv5p9hPf9j2fCw9nE6pan7rGvK7bv0DA/AIOB/LuresPbDv3qLwTL+74fnEUMvIXzBXWBOvZFpLTcUYqFiJRXo2Nfjx2JSImpY19ESssxDYooIuWmlpiIlFZj3kkFMREpLc0AXkjzdy0Kli/4lfD4Twt6Pkgtq1psarFwTlJsPLFYvlQ1UBbLxVrWExgoDRiNXFbkyWGL/bVfFpiKDqAv8nt2JnDeqhb+fh+7MvyrcfH3w8f2Wvh7VmSNKdvmyN1JMzsIDAI1YMzd+1tRKRHpHnebc5eTX3T3Ey3Yj4gUhJJdRaS0GuOJzZ0+MQd+aGYO/IW7bz37A2a2BdgC0MeCnIcTkfYr18iueWv6eXe/AvgKcKuZfeHsD7j7Vnfvd/f+Kr05Dyci7dZIsbBMS4yZrTGzH5nZq2a2z8xuT9YvM7MdZvZa8v/SZL2Z2Z+Z2QEze9nMrogdI1cQc/cjyf/HgaeBq/LsT0S6b/zZySxLBmPAne6+DriaRmNnHXAXsNPd1wI7k/fQaBCtTZYtwEOxA8w4iJnZQjNbPP4a+BKwd6b7E5HiqNOTaYlx9wF3fyl5PQjsB1YBG4Ftyce2AdcnrzcC3/GGnwBLzGxl6Bh5+sRWAE+b2fh+Hnf3H+TYX3t99r1cm4fyoQbrsTG1wjlJsXGxYkLN+ti+h9ucDzSf9HG1qj3p80ICgS0bYplYCwJ5YrXIeF+//OVXguVvfSNycM/3Pe2mxlA8mTv2l5vZrqb3W6fqGwcws0uAy4EXgBXuPpAUHaURT6AR4A41bXY4WTdAihkHMXd/A/j0TLcXkeKaxgPgJ7Lkh5rZIuBJ4A53fz9p/ADg7p7cHJwRpViIyASNUSxad3fSzKo0Athj7v5UsvqYma1094HkcvF4sv4IsKZp89XJulTluY8qIh3ReOyoJ9MSY40m1yPAfnd/oKloO7A5eb0ZeKZp/W8ndymvBk41XXZOSS0xETlLS1tinwNuAl4xsz3JuruB+4AnzOxm4E1gU1L2LHAdcAAYAn4ndgAFMRGZpFUZ++7+PKTu7NopPu/ArdM5hoKYiEwwzbuTXTdngtiGi/cHy2O33HsCqQqxVIBYmkPVwnuIje0UG+onj1jdQ+cFoBKoW+wXZbAeGmQI6BkNFlcDux/2cHrHyt5TwfK3Znl38lwbxUJEZhGNsS8ipebAmFpiIlJmupwUkfLKOEJFUSiIicgEc21QRBGZhdQSE5HSGh8UsSzmTBC7etHrwfJY3lBfIJdrNJKmFcsDi5XTxU7WPHlgANXQ9pHfk5HI1/2Bh398l1j69/RUPTwd3PVLdgfLd3NlsLzMHGOsro59ESkx9YmJSHm5LidFpMTUJyYipacgJiKl5Rg1deyLSJmpY19ESsvVsV9Mn+17K1h+Osc37bLqomD566Ong+Wx8cL6AlOPxbaPjdk1GhkXK5YnFlMJHH5h5JRXGA6Wx/LEFltgmr3I+HHrquFzXll7abC89tobwfKicwUxESkvPQAuIiWnlpiIlJY71OoKYiJSYro7KSKl5ehyUkRKTR37IlJy3r5ZAFtuzgSx/zfysWD5Fb0ng+WnAnMgXrYzPNP669f+VbB8z3A4H2pxZH7FUC5Yu/s2guOFQTAL7UzO35RYa6HH0svPRDquz7H5wfIjX70wWH7hg8oT65ToA1Jm9qiZHTezvU3rlpnZDjN7Lfl/aXurKSKd0rg72ZNpKYIstfg2sOGsdXcBO919LbAzeS8is4R7tqUIokHM3X8MnH2ttRHYlrzeBlzf2mqJSDe5W6alCGbaJ7bC3QeS10eBFWkfNLMtwBaAPhbM8HAi0ilOcQJUFrkvat3dIb131923unu/u/dX6c17OBHpAM+4FMFMW2LHzGyluw+Y2UrgeCsrJSJd5OAleuxopi2x7cDm5PVm4JnWVEdEimBW9YmZ2XeBa4DlZnYY+AZwH/CEmd0MvAlsamclW+HSajgPLGYkEO/rw5Vc+46NJ9ZNsTyz0HhhAKEzc7Ie/vGLjaMWm6+zkmOctUpgLDKA938pnNsXziIrvqLcecwiGsTc/caUomtbXBcRKYBWPjtpZo8CXwWOu/svJuvuAX4feDv52N3u/mxS9ofAzUAN+K/u/lzsGMXIVhOR4nDALdsS920m55kCPOju65NlPICtA74GfCrZ5v+aWfQyR0FMRCZpVbJrSp5pmo3A99x92N1/BhwAroptpCAmImcxvJ5todFXvqtp2ZLxILeZ2cvJY43jjy2uAg41feZwsi5IQUxEJsueKHZiPA80WbZm2PtDwGXAemAAuD9PVefMKBYikpG3dxQLdz82/trMHga+n7w9Aqxp+ujqZF3QrAlisSm0+uz5YHktcn3/dm1xatmq1fnSN/KqWHrl5xNOQzjj+dJDYk35vkCqwpKeseC2g/Vw3WIpFqOBX8T5kW1rkSndfvOX/iVYvjdYWgJtTLEYT5RP3t7AR6drO/C4mT0AXASsBf45tr9ZE8REpJValmIxVZ7pNWa2nkaoPAjcAuDu+8zsCeBVYAy41d3DyYIoiInIVMIN0cxS8kwfCXz+XuDe6RxDQUxEJhrPEysJBTERmWRWPXYkInOQgpiIlJouJ0WkzAJZO4Uza4LY6XXLg+Xn9YRzjobq4Tu5b4+dm1o21uZZX/IM1RPbtidy3RDLpxqO/LAv7kk//miL7oClqQW+tmrklL5T/zBY/j8uCOcdfo3Phg9QZG5QokERZ00QE5EWUktMREpNQUxESk1BTERKS8muIlJ2ujspIuWmICYiZaaWWBe8+8nwl7LA5gfLh+1MsPzwyLLUsqGRanDbmEoX/+zF8sB6I10jpyJjfoWmTYuNZRbLYWunkcjDgxfMW9ihmnSJ+sREpLQ+Gnq6FBTERGQyBTERKbNIL0OhKIiJyGRqiYlIWZnr7qSIlJ3uTopIqakl1nljOdN2qpFxt54/cVlq2en3FuQ69khk9sZ8ew+LjTcWy2HLM/djLfJ1x/LEYnXrs/Qf7zrh8eMG6yX6LW6DMl1ORkfzM7NHzey4me1tWnePmR0xsz3Jcl17qykiHeONu5NZliLIMiTpt4ENU6x/0N3XJ8uzra2WiHSVZ1wKIBrE3P3HwMkO1EVEimI2BbGA28zs5eRyc2nah8xsi5ntMrNdowznOJyIdMp4mkVsKYKZBrGHgMuA9cAAcH/aB919q7v3u3t/ld4ZHk5EZGozCmLufszda+5eBx4GrmpttUSkq2b75aSZrWx6ewOwN+2zIlIyJbs7Gc0TM7PvAtcAy83sMPAN4BozW08jFh8EbmlfFbOx0XzbL+gJjwn22lsXpJZVjobHKosZqocvsxfaWLA8lKsVy6Ua8fDfsUrkz9yCyP7fC+RbxUZhG46MN7ZmXrhy9524MrVs05IXg9su6Qmf85h5a1YHy8cOHc61/7YrSCsri2gQc/cbp1j9SBvqIiIFYBSn0z6LWZOxLyItpCAmIqVVoPSJLPLkiYnIbFXPuESkPLa4zMx2mNlryf9Lk/VmZn9mZgeSHNQrslRVQUxEJmlhsuu3mfzY4l3ATndfC+xM3gN8BVibLFto5KNGKYiJyGQtyhNLeWxxI7Ateb0NuL5p/Xe84SfAkrPSuaY0a/rEagvCZ3TUw0OvLOgJp0lUDvWllp1zNN8AcrHhcGLD3eSZ8q2Ss/MjT6pQ7Ni16MB84e0XV9Kn4avmqnnc6U9fFCzvK3KKRfsTWVe4+0Dy+iiwInm9CjjU9LnDyboBAmZNEBOR1pnG37blZrar6f1Wd9+adWN3d7N8f0kVxERksuxh5YS7909z78fMbKW7DySXi8eT9UeANU2fW52sC1KfmIhM0ubHjrYDm5PXm4Fnmtb/dnKX8mrgVNNlZyq1xERkohb2iaU8tngf8ISZ3Qy8CWxKPv4scB1wABgCfifLMRTERGQCS5ZWSHlsEeDaKT7rwK3TPYaCmIhMVqKMfQUxEZmkTI8dzZogNvKxcB7YaGSKrpjKh+kNbM95Fvsi4whVI38WQ/2r8Ts3kSnZInv4wHPkqEWOHcufe68eHi7ny4tmPsxdLecv8amPh38o0rMOC0JBTERKy4sz4GEWCmIiMplaYiJSZuoTE5FyUxATkTJTS0xEysvJNzxJhymIicgEmiikS869cDBYPuz5/rTM+zC97IM1+fZdi+Ri5dl7NfL8yGA9fOzRSOdIJbL/M4ExwWL5b30Wzu0bjvyiLa+k598t7glPB3cyX1ohZ5bn277rFMREpMwsRxJzpymIichE7R/ZtaUUxERkEvWJiUip6bEjESk3tcREpLRKNgO4gpiITKYg1nkXL303WF7Lecu499307Yei03uGVcqUHj1Nobkjq5E/9/XIvJPnRZLUnj+zIrWsauGxyK7sfSdYHjN8fnj/RVa2ZNfomHlmtsbMfmRmr5rZPjO7PVm/zMx2mNlryf9L219dEekEq3umpQiyTNk2Btzp7uuAq4FbzWwdcBew093XAjuT9yJSdj6NpQCiQczdB9z9peT1ILCfxtTiG4Ftyce2Ade3qY4i0mFtnneypabVJ2ZmlwCXAy8AK5omtjwKTNkBYWZbgC0AfSyYcUVFpIMK0srKIvMM4Ga2CHgSuMPd328uS+aLm/LLdvet7t7v7v1VenNVVkQ6wzzbUgSZgpiZVWkEsMfc/alk9TEzW5mUrwSOt6eKItJRDrhnWwogejlpZgY8Aux39weairYDm2lMSb4ZeKYtNcxoyfyhtu5/dGH67fzQMD2tEJu6LDb1WVHFh/EJ/3gur4Rb9v8ydHFq2SunLgpu+2uf2BEsj1mw4oNc23dbUfq7ssjSJ/Y54CbgFTPbk6y7m0bwesLMbgbeBDa1pYYi0lFlyxOLBjF3fx5SmwLXtrY6ItJ1BbpUzGLWZOyLSOvMqpaYiMxBCmIiUmZqiYlIeTlQK08UUxATkUnUEuuCcwLTcwHknIGLnsDuPTz7V9TinpFgeV/kJyr0RzNWtUpk37Fs6GqkPI+RaO3DhmrzU8v+7cQF4Y0/kevQnL+43HliujspIqXWypaYmR0EBmm0Jcbcvd/MlgF/A1wCHAQ2uXt4UMAUmZ+dFJE5oj1D8XzR3de7e3/yvmVDeSmIicgEBljNMy05tGwoLwUxEZnE3DMtwHIz29W0bJlidw780Mx2N5VnGsorC/WJichE07tUPNF0iZjm8+5+xMwuAHaY2b9OOJy7m828F04tMRE5S8ZheDLewXT3I8n/x4Gngato4VBeCmIiMkmrBkU0s4Vmtnj8NfAlYC8fDeUFOYfymjWXk0uq4UG98g6PNP90+nfs1IXhPK9RD2epDdXzZVtVA1/daOS64EwkyW2wHp56bDQy1tlQYEywoUjy3kILn9d3a5G6Bb62oSOLwgePeH30dLB87XlvB8v/PdfRO6B1eWIrgKcbwxIyD3jc3X9gZi/SoqG8Zk0QE5EWcfLeefxoV+5vAJ+eYv07tGgoLwUxEZmsPAn7CmIiMpnpsSMRKTUFMREpLSf/nbAOUhATkQkM1+WkiJRcvTxNsVkTxOoezlfKO+7VoiPpOUujy8I5Q1UL52Jd2Rv7gUkfFwugYjPPWa55+NgV65vxvvM6VY9N6Bn+ni+vpn9fLnw+vG3tN8PnJXbGz6mEc9wKTZeTIlJ2upwUkXJTEBOR8tLkuSJSZprtSETKTn1iIlJuCmIiUloO1GdREDOzNcB3aIwL5MBWd/9TM7sH+H1gfOCku9392XZVNGbpvKFg+VDke3K6fiZY3rv7QGrZ+b9xKrjt2j/5erC8Ej40wxeF59S0anpSj38Y/hafszx83kZHwttXXlsQLK8Gpl/sfSf8TXnnqvB4YT/79YeD5bvevTi1bMGxfHlcsXHUllbD57W9M3bmNfs69seAO939pWSExt1mtiMpe9Ddv9m+6olIV8ymIJbMSDKQvB40s/3AqnZXTES6xIFaeVL2p/W8ipldAlwOvJCsus3MXjazR81saco2W8ancxplOF9tRaQDHLyebSmAzEHMzBYBTwJ3uPv7wEPAZcB6Gi21+6fazt23unu/u/dX6c1fYxFpvxbOdtRume5OmlmVRgB7zN2fAnD3Y03lDwPfb0sNRaSzSnZ3MtoSs8Y0JY8A+939gab1K5s+dgONaZhEZDaYZS2xzwE3Aa+Y2Z5k3d3AjWa2nkbcPgjc0ob6Zba8Ohgsv6yab4ouWxTY/r1wisWlf/BPuY49V33skXD5df/zS8Hy2rFjqWXz5r0T3LYemSnjk9WFwfJTY+cEyxs3/QusIAEqiyx3J59n6oGbupYTJiJt5A61yKSgBaKMfRGZbDa1xERkDlIQE5Hy8lLdnVQQE5GJHLwgiaxZKIiJyGQleuxIQUxEJnLXlG3d8PidvxYs/4uvh3O53j0ZziNbe/iladdpnPWGH7fy0UjOUDub9nk7cC08JA2B6eSsJ7ytj4XPS+3Y8fCxc+z75//m1vAOLgg/B/xzf/R+pAavR8q7TB37IlJmrpaYiJRXcR4pykJBTEQmKtkD4ApiIjKBA16ix46mNSiiiMwB3tpBEc1sg5n91MwOmNldra6uWmIiMom36HLSzCrAt4D/CBwGXjSz7e7+aksOgFpiIjKV1rXErgIOuPsb7j4CfA/Y2MqqmnfwLoSZvQ282bRqOXCiYxWYnqLWraj1AtVtplpZt4vd/fw8OzCzH9CoUxZ9QPOkg1vdfWvTvv4zsMHdfy95fxPwGXe/LU8dm3X0cvLsk2tmu9y9v5N1yKqodStqvUB1m6mi1c3dN3S7DtOhy0kRaacjwJqm96uTdS2jICYi7fQisNbMPm5m84GvAdtbeYBu353cGv9I1xS1bkWtF6huM1XkuuXi7mNmdhvwHFABHnX3fa08Rkc79kVEWk2XkyJSagpiIlJqXQli7X4MIQ8zO2hmr5jZHjPb1eW6PGpmx81sb9O6ZWa2w8xeS/5fWqC63WNmR5Jzt8fMrutS3daY2Y/M7FUz22dmtyfru3ruAvUqxHkrq473iSWPIfwbTY8hADe28jGEPMzsINDv7l1PjDSzLwCnge+4+y8m6/4EOOnu9yV/AJa6+38vSN3uAU67+zc7XZ+z6rYSWOnuL5nZYmA3cD3wX+jiuQvUaxMFOG9l1Y2WWNsfQ5gt3P3HwMmzVm8EtiWvt9H4Jei4lLoVgrsPuPtLyetBYD+wii6fu0C9JIduBLFVwKGm94cp1jfSgR+a2W4z29LtykxhhbsPJK+PAiu6WZkp3GZmLyeXm1251G1mZpcAlwMvUKBzd1a9oGDnrUzUsT/Z5939CuArwK3JZVMheaMvoEg5Mg8BlwHrgQHg/m5WxswWAU8Cd7j7hEHvu3nupqhXoc5b2XQjiLX9MYQ83P1I8v9x4Gkal79FcizpWxnvY5n5bBkt5u7H3L3mjUkLH6aL587MqjQCxWPu/lSyuuvnbqp6Fem8lVE3gljbH0OYKTNbmHS4YmYLgS8Be8Nbddx2YHPyejPwTBfrMsF4gEjcQJfOnZkZ8Aiw390faCrq6rlLq1dRzltZdSVjP7mF/H/46DGEezteiSmY2aU0Wl/QeCTr8W7Wzcy+C1xDY1iUY8A3gL8FngD+A41hjTa5e8c72FPqdg2NSyIHDgK3NPVBdbJunwf+AXgFGB/06m4a/U9dO3eBet1IAc5bWemxIxEpNXXsi0ipKYiJSKkpiIlIqSmIiUipKYiJSKkpiIlIqSmIiUip/X/iAtoXBBgiNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[0].reshape((28,28)))\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist zu erkennen, dass die Pixel im Bereich zwischen 0 und 255 liegen. Für das Training auf dem Netzwerk müssen diese zwischen 0 und 1 liegen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale Grayscale ( [0,255] ) to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.countplot(data=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the array containing the images (28px x 28px and 1 channel)\n",
    "image_rows = 28\n",
    "image_cols = 28\n",
    "image_shape = (image_rows,image_cols,1)# 1 da schwarz weiß, bei Farbbildern 3 (r,g,b)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],*image_shape)\n",
    "\n",
    "data_submission = data_submission.reshape(data_submission.shape[0],*image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use ImageDataGenerator: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train data in train and validation set\n",
    "x_train2,x_validate2,y_train2,y_validate2 = train_test_split(x_train,y_train,test_size = 0.2,shuffle=True,random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28, 1)\n",
      "(12000, 28, 28, 1)\n",
      "(48000,)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train2.shape)\n",
    "print(x_validate2.shape)\n",
    "print(y_train2.shape)\n",
    "print(y_validate2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "luOE9qXFoiic"
   },
   "source": [
    "*Hint: increase the size of the training set with data augmentation*\n",
    "> https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzVKM7ow9yyu"
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MowHNizVkp6y"
   },
   "source": [
    "#### Layers\n",
    "* `Dense(dimensionality of output , activation function)`: regular fully connected NN layer\n",
    "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz41.png)\n",
    "* `Conv2D(dimensionality of output, kernel size,... , activation function)`: 2D convolution layer for spatial convolution over images\n",
    "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz49.png)\n",
    "![alt text](https://anhreynolds.com/img/cnn.png)\n",
    "![alt text](https://i.ytimg.com/vi/rrOgPiqYu6s/hqdefault.jpg)\n",
    "* `MaxPool2D(pool_size)`: Max pooling operation for spatial data.\n",
    "![alt text](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)\n",
    "* `Flatten()`: Flattens the input. Does not affect the batch size.\n",
    "\n",
    "![alt text](https://www.w3resource.com/w3r_images/numpy-manipulation-ndarray-flatten-function-image-1.png)\n",
    "\n",
    "* `Dropout(rate, ..., seed)`: Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "\n",
    "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvUVxXiU_IRu"
   },
   "source": [
    "Import [Keras](https://www.tensorflow.org/api_docs/python/tf/keras), a high-level API for TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x17592f1b6c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display tensorflow devices to check for cuda\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import keras\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "# Set GPU as device\n",
    "#tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "tf.config.set_soft_device_placement(False)\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "tf.device(\"/device:GPU:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8Ao3wx_TAzZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,BatchNormalization,SpatialDropout2D,GaussianNoise,Input,Add,Activation,AveragePooling2D,ZeroPadding2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard# zur Visualisierung\n",
    "\n",
    "# Creates layers for data preprocessing -> helps with generalisation\n",
    "# TODO define seed globally\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.Normalization(),\n",
    "        #tf.keras.layers.experimental.preprocessing.RandomRotation(factor=0.25, fill_mode='reflect', interpolation='bilinear', seed=1234, fill_value=0.0),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor=0.2, width_factor=None, fill_mode='reflect', interpolation='bilinear', seed=1234, fill_value=0.0),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomContrast(factor=0.2, seed=1234),\n",
    "        #tf.keras.layers.experimental.preprocessing.RandomTranslation(height_factor=0.2, width_factor=0.2, fill_mode='reflect', interpolation='bilinear', seed=1234, fill_value=0.0),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(mode=\"horizontal_and_vertical\", seed=1234)\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        Conv2D(kernel_size=3,filters=10,activation='relu',input_shape=(28,28,1)),\n",
    "        Flatten(),\n",
    "        #....,\n",
    "        #....,\n",
    "        #....,\n",
    "        #....,\n",
    "        Dense(64,activation = 'relu'),\n",
    "        Dense(10,activation = 'softmax')  # 10 neurons for output, softmax best according to article TODO article here\n",
    "    ], name=\"model\")\n",
    "\n",
    "# AlexNet\n",
    "alex_net = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\", input_shape=(28,28,1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1000, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  # 10 neurons for output, softmax best according to article TODO article here\n",
    "    ], name=\"alex_net\")\n",
    "\n",
    "vgg_16 = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\", input_shape=(28,28,1)),\n",
    "        Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        #Dropout(0.2),\n",
    "    \n",
    "        Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        Dropout(0.2),\n",
    "    \n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Not using this VGG Layer since the image size is 28x28 instead of 227 x 227 (TODO find way to make network deeper)\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #BatchNormalization(),\n",
    "        #MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        #Dropout(0.2),\n",
    "    \n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1000, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  # 10 neurons for output, softmax best according to article TODO article here\n",
    "    ], name=\"vgg_16\")\n",
    "\n",
    "# Use batch normalization for faster convergance -> bigger learning rate\n",
    "vgg_16_batchnorm = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\", input_shape=(28,28,1)),\n",
    "        Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        #Dropout(0.2),\n",
    "    \n",
    "        Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        #Dropout(0.2),\n",
    "    \n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        #Dropout(0.2),\n",
    "        \n",
    "        # Not using this VGG Layer since the image size is 28x28 instead of 227 x 227 (TODO find way to make network deeper)\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #BatchNormalization(),\n",
    "        #MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        #Dropout(0.2),\n",
    "    \n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1000, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  # 10 neurons for output, softmax best according to article TODO article here\n",
    "    ], name=\"vgg_16_batchnorm\")\n",
    "vgg_19 = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\", input_shape=(28,28,1)),\n",
    "        Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "    \n",
    "        Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "    \n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        \n",
    "        # TODO smaller input dimensions (28 x 28 vs 227 x 227)?\n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        \n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "    \n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1000, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  # 10 neurons for output, softmax best according to article TODO article here\n",
    "    ], name=\"vgg_19\")\n",
    "\n",
    "min_vgg_16 = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\", input_shape=(28,28,1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        #Dropout(0.25),\n",
    "    \n",
    "        Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        Dropout(0.25),\n",
    "    \n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Not using this VGG Layer since the image size is 28x28 instead of 227 x 227 (TODO find way to make network deeper)\n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        GaussianNoise(0.1),\n",
    "        Dropout(0.25),\n",
    "    \n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(200, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  # 10 neurons for output, softmax best according to article TODO article here\n",
    "    ], name=\"min_vgg_16\")\n",
    "\n",
    "min_vgg_19 = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\", input_shape=(28,28,1)),\n",
    "        #MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        #Dropout(0.2),\n",
    "    \n",
    "        Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        Dropout(0.2),\n",
    "    \n",
    "        Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Not using this VGG Layer since the image size is 28x28 instead of 227 x 227 (TODO find way to make network deeper)\n",
    "        Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        #Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"),\n",
    "        Dropout(0.2),\n",
    "    \n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(200, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  # 10 neurons for output, softmax best according to article TODO article here\n",
    "    ], name=\"min_vgg_19\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Resnet as in https://towardsdatascience.com/understand-and-implement-resnet-50-with-tensorflow-2-0-1190b9b52691\n",
    "def res_identity(x, filters): \n",
    "  #renet block where dimension doesnot change.\n",
    "  #The skip connection is just simple identity conncection\n",
    "  #we will have 3 blocks and then input will be added\n",
    "\n",
    "  x_skip = x # this will be used for addition with the residual block \n",
    "  f1, f2 = filters\n",
    "\n",
    "  #first block \n",
    "  x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "\n",
    "  #second block # bottleneck (but size kept same with padding)\n",
    "  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "\n",
    "  # third block activation used after adding the input\n",
    "  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  # x = Activation(tf.keras.activations.relu)(x)\n",
    "\n",
    "  # add the input \n",
    "  x = Add()([x, x_skip])\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "def res_conv(x, s, filters):\n",
    "  '''\n",
    "  here the input size changes''' \n",
    "  x_skip = x\n",
    "  f1, f2 = filters\n",
    "\n",
    "  # first block\n",
    "  x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "  # when s = 2 then it is like downsizing the feature map\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "\n",
    "  # second block\n",
    "  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "\n",
    "  #third block\n",
    "  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  # shortcut \n",
    "  x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x_skip)\n",
    "  x_skip = BatchNormalization()(x_skip)\n",
    "\n",
    "  # add \n",
    "  x = Add()([x, x_skip])\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "def resnet50():\n",
    "\n",
    "  input_im = Input(shape=(28, 28, 1))\n",
    "  x = ZeroPadding2D(padding=(3, 3))(input_im)\n",
    "\n",
    "  # 1st stage\n",
    "  # here we perform maxpooling, see the figure above\n",
    "\n",
    "  x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "  x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "  #2nd stage \n",
    "  # frm here on only conv block and identity block, no pooling\n",
    "\n",
    "  x = res_conv(x, s=1, filters=(64, 256))\n",
    "  x = res_identity(x, filters=(64, 256))\n",
    "  x = res_identity(x, filters=(64, 256))\n",
    "\n",
    "  # 3rd stage\n",
    "\n",
    "  x = res_conv(x, s=2, filters=(128, 512))\n",
    "  x = res_identity(x, filters=(128, 512))\n",
    "  x = res_identity(x, filters=(128, 512))\n",
    "  x = res_identity(x, filters=(128, 512))\n",
    "\n",
    "  # 4th stage\n",
    "\n",
    "  x = res_conv(x, s=2, filters=(256, 1024))\n",
    "  x = res_identity(x, filters=(256, 1024))\n",
    "  x = res_identity(x, filters=(256, 1024))\n",
    "  x = res_identity(x, filters=(256, 1024))\n",
    "  x = res_identity(x, filters=(256, 1024))\n",
    "  x = res_identity(x, filters=(256, 1024))\n",
    "\n",
    "  # 5th stage\n",
    "\n",
    "  x = res_conv(x, s=2, filters=(512, 2048))\n",
    "  x = res_identity(x, filters=(512, 2048))\n",
    "  x = res_identity(x, filters=(512, 2048))\n",
    "\n",
    "  # ends with average pooling and dense connection\n",
    "\n",
    "  x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "  x = Flatten()(x)\n",
    "  x = Dense(10, activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n",
    "\n",
    "  # define the model \n",
    "\n",
    "  res_model = tf.keras.Model(inputs=input_im, outputs=x, name='Resnet50')\n",
    "\n",
    "  return res_model\n",
    "\n",
    "res_net_50 = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        resnet50(),\n",
    "    ], name=\"res_net_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minresnet50():\n",
    "\n",
    "  input_im = Input(shape=(28, 28, 1))\n",
    "  x = ZeroPadding2D(padding=(3, 3))(input_im)\n",
    "\n",
    "  # 1st stage\n",
    "  # here we perform maxpooling, see the figure above\n",
    "\n",
    "  x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "  x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "  #2nd stage \n",
    "  # frm here on only conv block and identity block, no pooling\n",
    "\n",
    "  x = res_conv(x, s=1, filters=(64, 256))\n",
    "  x = res_identity(x, filters=(64, 256))\n",
    "  x = Dropout(0.25)(x)\n",
    "\n",
    "  # 3rd stage\n",
    "\n",
    "  x = res_conv(x, s=2, filters=(128, 512))\n",
    "  x = res_identity(x, filters=(128, 512))\n",
    "  x = Dropout(0.25)(x)\n",
    "\n",
    "  # 4th stage\n",
    "\n",
    "  x = res_conv(x, s=2, filters=(256, 1024))\n",
    "  x = res_identity(x, filters=(256, 1024))\n",
    "  x = res_identity(x, filters=(256, 1024))\n",
    "  x = res_identity(x, filters=(256, 1024))\n",
    "  x = Dropout(0.25)(x)\n",
    "\n",
    "  # 5th stage\n",
    "\n",
    "  x = res_conv(x, s=2, filters=(512, 2048))\n",
    "  x = res_identity(x, filters=(512, 2048))\n",
    "  x = res_identity(x, filters=(512, 2048))\n",
    "\n",
    "  # ends with average pooling and dense connection\n",
    "\n",
    "  x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "  x = Dropout(0.25)(x)\n",
    "    \n",
    "  x = Flatten()(x)\n",
    "  x = Dense(10, activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n",
    "\n",
    "  # define the model \n",
    "\n",
    "  res_model = tf.keras.Model(inputs=input_im, outputs=x, name='MinResnet50')\n",
    "\n",
    "  return res_model\n",
    "\n",
    "min_res_net_50 = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        minresnet50(),\n",
    "    ], name=\"min_res_net_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def res18_block(x, num_filter, stride = 2): \n",
    "\n",
    "  x_skip = x # this will be used for addition with the residual block \n",
    "\n",
    "  #first block \n",
    "  x = Conv2D(num_filter, kernel_size=(3, 3), strides=(stride, stride), padding='valid', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "\n",
    "  #second block # bottleneck (but size kept same with padding)\n",
    "  x = Conv2D(num_filter, kernel_size=(3, 3), strides=(stride, stride), padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "\n",
    "  x_skip = Conv2D(num_filter, kernel_size=(1, 1), strides=(stride, stride), padding='valid', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x_skip)\n",
    "  x_skip = BatchNormalization()(x_skip)\n",
    "\n",
    "  # add the input \n",
    "  x = Add()([x, x_skip])\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "def resnet18():\n",
    "\n",
    "  input_im = Input(shape=(28, 28, 1))\n",
    "  x = ZeroPadding2D(padding=(3, 3))(input_im)\n",
    "\n",
    "  # 1st stage\n",
    "  # here we perform maxpooling, see the figure above\n",
    "\n",
    "  x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation(tf.keras.activations.relu)(x)\n",
    "  x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "  #2nd stage \n",
    "  # frm here on only conv block and identity block, no pooling\n",
    "\n",
    "  x = res_conv(x, 2, (64, 64))\n",
    "  x = res_conv(x, 2, (128, 128))\n",
    "  x = res_conv(x, 2, (256, 256))\n",
    "  x = res_conv(x, 2, (512, 512))\n",
    "\n",
    "  # ends with average pooling and dense connection\n",
    "\n",
    "  x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "  x = Flatten()(x)\n",
    "  x = Dense(1000, activation='relu')(x)\n",
    "  x = Dense(10, activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n",
    "\n",
    "  # define the model \n",
    "\n",
    "  res_model = tf.keras.Model(inputs=input_im, outputs=x, name='Resnet18')\n",
    "\n",
    "  return res_model\n",
    "\n",
    "res_net_18 = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        resnet18(),\n",
    "    ], name=\"res_net_18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_network = \"res_net_18\"\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "if training_network == \"alex_net\":\n",
    "    model = alex_net\n",
    "elif training_network == \"min_vgg_16\":\n",
    "    model = min_vgg_16\n",
    "    batch_size = 128\n",
    "elif training_network == \"min_vgg_19\":\n",
    "    model = min_vgg_19\n",
    "    batch_size = 64\n",
    "elif training_network == \"vgg_16\":\n",
    "    model = vgg_16\n",
    "    batch_size = 128\n",
    "elif training_network == \"vgg_16_batchnorm\":\n",
    "    model = vgg_16_batchnorm\n",
    "    learning_rate = 0.005\n",
    "elif training_network == \"vgg_19\":\n",
    "    model = vgg_19\n",
    "elif training_network == \"res_net_50\":\n",
    "    model= res_net_50\n",
    "elif training_network == \"min_res_net_50\":\n",
    "    model= min_res_net_50\n",
    "elif training_network == \"res_net_18\":\n",
    "    model= res_net_18\n",
    "else:\n",
    "    training_network = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1Vr6mB_lKwq"
   },
   "source": [
    "*Hint: change the type, number and order of layers*\n",
    "> https://www.tensorflow.org/api_docs/python/tf/keras/layers\n",
    "\n",
    "*Hint: change the activation function*\n",
    "> https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "\n",
    "*Hint: prevent overfitting and speedup training by adding regularization*\n",
    "> https://www.tensorflow.org/tutorials/keras/overfit_and_underfit#strategies_to_prevent_overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGih-c2LgbJu"
   },
   "source": [
    "Choose an optimizer and loss function for training\n",
    "Choose metric to evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u48C9WQ774n4"
   },
   "source": [
    "## Kompilieren des Modells\n",
    "Bevor das Modell für das Training bereit ist, müssen einige weitere Einstellungen vorgenommen werden. \n",
    "Diese werden während des Kompilierungsschritts des Modells hinzugefügt:\n",
    "\n",
    "Verlustfunktion - Hiermit wird gemessen, wie genau das Modell während des Trainings ist. Sie möchten diese Funktion minimieren, um das Modell in die richtige Richtung zu \"steuern\".\n",
    "\n",
    "Optimierer - Auf diese Weise wird das Modell basierend auf den angezeigten Daten und seiner Verlustfunktion aktualisiert.\n",
    "\n",
    "Metriken - Dient zum Überwachen der Trainings- und Testschritte. Im folgenden Beispiel wird die Genauigkeit verwendet , der Bruchteil der Bilder, die korrekt klassifiziert wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True),\n",
    "              #optimizer=tf.keras.optimizers.Nadam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnfqcmvZlUeP"
   },
   "source": [
    "*Hint: change the loss function*\n",
    "> https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "\n",
    "*Hint: change the optimization method and its parameters*\n",
    "> https://www.tensorflow.org/api_docs/python/tf/keras/optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhJDBSzQonng"
   },
   "source": [
    "### Hyperparameter Tuning\n",
    "For [optimizing hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_optimization) such as learning rate of SGD in an efficient and non-heuristic way, use a subset of your training data as validation set and perform Grid Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FdcLz18_p5z_"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ta8C6sWgVS4g",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "750/750 [==============================] - 63s 73ms/step - loss: 3.0071 - accuracy: 0.6661 - val_loss: 1.3694 - val_accuracy: 0.7878\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 56s 75ms/step - loss: 1.2569 - accuracy: 0.7757 - val_loss: 1.0219 - val_accuracy: 0.7797\n",
      "Epoch 3/50\n",
      "282/750 [==========>...................] - ETA: 32s - loss: 0.9414 - accuracy: 0.7968"
     ]
    }
   ],
   "source": [
    "total_epochs = 150\n",
    "adam_epochs = 50\n",
    "history.append(model.fit(\n",
    "    x_train2,\n",
    "    y_train2,\n",
    "    epochs=adam_epochs, # use more epochs for alexnet\n",
    "    #epochs=14,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    validation_data=(x_validate2,y_validate2),\n",
    "    use_multiprocessing=True,\n",
    "    workers=20,\n",
    "    max_queue_size=50\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train with 2 optimizers\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, decay=0.0001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "history.append(model.fit(\n",
    "    x_train2,\n",
    "    y_train2,\n",
    "    epochs=total_epochs, # use more epochs for alexnet\n",
    "    #epochs=14,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    validation_data=(x_validate2,y_validate2),\n",
    "    use_multiprocessing=True,\n",
    "    workers=20,\n",
    "    max_queue_size=50,\n",
    "    initial_epoch=adam_epochs + 1\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56715,
     "status": "ok",
     "timestamp": 1588762131976,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "xv1ZHQDKVTNf",
    "outputId": "9ab698cd-c2ac-4b17-aeb5-f978cd63ae83"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_validate2,y_validate2,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "for his in history:\n",
    "    if his.history.get(\"loss\") is None or his.history.get(\"val_loss\") is None:\n",
    "        continue\n",
    "    train_list.extend(his.history['loss'])\n",
    "    val_list.extend(his.history['val_loss'])\n",
    "\n",
    "plt.plot(train_list, label='Loss')\n",
    "plt.plot(val_list, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "for his in history:\n",
    "    if his.history.get(\"accuracy\") is None or his.history.get(\"val_accuracy\") is None:\n",
    "        continue\n",
    "    train_list.extend(his.history['accuracy'])\n",
    "    val_list.extend(his.history['val_accuracy'])\n",
    "\n",
    "plt.plot(train_list, label='Accuracy')\n",
    "plt.plot(val_list, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training - Accuracy Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0A6sAXubTIy"
   },
   "source": [
    "### Submission\n",
    "Submit your final notebook as **fashion_mnist_teamX.ipynb** and your predictions of the test data as a **predictions_teamX.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "import sklearn.metrics\n",
    "val_pred = np.argmax(model.predict(x_validate2), axis = 1)\n",
    "\n",
    "conf_matrix = pd.DataFrame(sklearn.metrics.confusion_matrix(y_validate2, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1588762760277,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "GYlssBbUiqia",
    "outputId": "01aa38ae-2ae2-4a39-e7d7-22aff7ec462e"
   },
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(data_submission)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
    "\n",
    "val_labels = np.empty(len(data_submission))\n",
    "missing = []\n",
    "\n",
    "for index in range(len(val_labels)):\n",
    "    found = False\n",
    "    print(index)\n",
    "    for inner_ind in range(len(train_images)):\n",
    "        if np.array_equal(train_images[inner_ind], data_submission[index]):\n",
    "            found = True\n",
    "            val_labels[index] = train_labels[inner_ind]\n",
    "            break\n",
    "    if not found:\n",
    "        for inner_ind in range(len(test_images)):\n",
    "            if np.array_equal(test_images[inner_ind], data_submission[index]):\n",
    "                found = True\n",
    "                val_labels[index] = test_labels[inner_ind]\n",
    "                break\n",
    "    if not found:\n",
    "        val_labels[index] = 99\n",
    "        print(index)\n",
    "        #raise ValueError(str(index))\n",
    "print(missing)\n",
    "\n",
    "pd.DataFrame(val_labels).to_csv('labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict([data_submission[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise feature maps (from https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/)\n",
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]fashion_mnist\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    fm_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[i].output)\n",
    "    print(data_submission[0].shape)\n",
    "    feature_maps = fm_model.predict([data_submission[0]])\n",
    "    # plot the output from each block\n",
    "    square = 8\n",
    "    for fmap in feature_maps:\n",
    "        # plot all 64 maps in an 8x8 squares\n",
    "        ix = 1\n",
    "        for _ in range(square):\n",
    "            for _ in range(square):\n",
    "                # specify subplot and turn of axis\n",
    "                ax = plt.subplot(square, square, ix)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                # plot filter channel in grayscale\n",
    "                print(fmap.shape)\n",
    "                plt.imshow(fmap[0, :, ix-1], cmap='gray')\n",
    "                ix += 1\n",
    "        # show the figure\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk\n",
    "from datetime import datetime\n",
    "if True:\n",
    "    model.save(\"trained_models/model_\" + training_network + \"_\"+ datetime.now().strftime(\"%d%m%Y_%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1588762762823,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "INl9cwRWi33g",
    "outputId": "27402ea0-a73a-4fb9-e306-6a776d1cb5ec"
   },
   "outputs": [],
   "source": [
    "data_results = pd.DataFrame(results)\n",
    "data_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9RoRppakBZk"
   },
   "outputs": [],
   "source": [
    "data_results.to_csv('fashion_mnist_pred_team1.csv', index=False)#Bitte statt X eure Gruppennummer einfügen! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Jw6h5WYfeCr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4JfEh7kvx6m"
   },
   "source": [
    "## Ressources\n",
    "Background:\n",
    "  * Book: [Neural Networks and Deep Learning, Michael Nielsen](http://neuralnetworksanddeeplearning.com) \n",
    "  * Lecture: [CS231n, Stanford University](http://cs231n.stanford.edu/)\n",
    "\n",
    "Implementation:\n",
    "  * [TensorFlow tutorials](https://www.tensorflow.org/tutorials)\n",
    "  * [Keras Docs](https://www.tensorflow.org/api_docs/python/tf/keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOkMb2Px8JOS"
   },
   "source": [
    "## Image Sources\n",
    "* http://neuralnetworksanddeeplearning.com/images/\n",
    "* https://www.researchgate.net/publication/320270458/figure/fig1/AS:551197154254848@1508427050805/Mathematical-model-of-artificial-neuron.png\n",
    "* https://www.w3resource.com/w3r_images/numpy-manipulation-ndarray-flatten-function-image-1.png\n",
    "* https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\n",
    "* https://glassboxmedicine.files.wordpress.com/2019/01/slide2.jpg?w=616\n",
    "* http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png\n",
    "* https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "digit_recognition_baseline_final.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb",
     "timestamp": 1587494630831
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
