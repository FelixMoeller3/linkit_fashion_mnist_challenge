{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDb1ilq8WZ5r"
   },
   "source": [
    "# Fashion MNIST Data Science Challenge: Neural Networks and Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels\n",
    "\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "<li>0 T-shirt/top </li>\n",
    "<li>1 Trouser</li>\n",
    "<li>2 Pullover </li>\n",
    "<li>3 Dress </li>\n",
    "<li>4 Coat </li>\n",
    "<li>5 Sandal</li>\n",
    "<li>6 Shirt </li>\n",
    "<li>7 Sneaker </li>\n",
    "<li>8 Bag </li>\n",
    "<li>9 Ankle boot </li>\n",
    "--------------------------\n",
    "\n",
    "Each row is a separate image\n",
    "\n",
    "Column 1 is the class label.\n",
    "Remaining columns are pixel numbers (784 total).\n",
    "Each value is the darkness of the pixel (1 to 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iQeajjDXCiS"
   },
   "source": [
    "## **Fashion Mnist Classification**\n",
    "Let's build a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4071,
     "status": "ok",
     "timestamp": 1588762079160,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "xTS0YM-sOJyf",
    "outputId": "5e739605-4550-4146-a90e-bd809119b88e"
   },
   "outputs": [],
   "source": [
    "#basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z33CvoON9uoF"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Fptwh8hO62p"
   },
   "outputs": [],
   "source": [
    "#1. Get the file\n",
    "data_train = pd.read_csv('train.csv')\n",
    "data_validate = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>150</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>169</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      0       0       0       0       0       0       0       0       1   \n",
       "1      0       0       0       0       0       0       0       1       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      0       0       0       0       0       0       0       0       0   \n",
       "4      1       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     123  ...         0         0         0         0       127       150   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...       169        43         0         0         0         0   \n",
       "3       0  ...       129        37         0         0         0         0   \n",
       "4       0  ...         2         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0        28         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 2 models one for shoes, bags and troussers\n",
    "data_easy_to_classify = data_train.copy()\n",
    "\n",
    "# Exchange labels of T-Shirts, pullovers, dresses, coats and shirts with common label\n",
    "#data_easy_to_classify.loc[data_easy_to_classify['label'] == 0, 'label'] = 0  # t-shirt\n",
    "data_easy_to_classify.loc[data_easy_to_classify['label'] == 2, 'label'] = 0  # pullover\n",
    "data_easy_to_classify.loc[data_easy_to_classify['label'] == 3, 'label'] = 0  # dress\n",
    "data_easy_to_classify.loc[data_easy_to_classify['label'] == 6, 'label'] = 0  # shirt\n",
    "\n",
    "# reorganize labels\n",
    "new_easy_label_dict = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "    4: 2,\n",
    "    5: 3,\n",
    "    6: 0,\n",
    "    7: 4,\n",
    "    8: 5,\n",
    "    9: 6\n",
    "}\n",
    "\n",
    "easy_trafo_dict = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 4,\n",
    "    3: 5,\n",
    "    4: 7,\n",
    "    5: 8,\n",
    "    6: 9\n",
    "}\n",
    "\n",
    "# TODO needed?\n",
    "#data_easy_to_classify.loc[data_easy_to_classify['label'] == 1, 'label'] = 1  # trouser\n",
    "# data_easy_to_classify.loc[data_easy_to_classify['label'] == 4, 'label'] = 2  # coat\n",
    "# data_easy_to_classify.loc[data_easy_to_classify['label'] == 5, 'label'] = 3  # sandal\n",
    "# data_easy_to_classify.loc[data_easy_to_classify['label'] == 7, 'label'] = 4  # sneaker\n",
    "# data_easy_to_classify.loc[data_easy_to_classify['label'] == 8, 'label'] = 5  # bag\n",
    "# data_easy_to_classify.loc[data_easy_to_classify['label'] == 9, 'label'] = 6  # ankle boot\n",
    "\n",
    "data_hard_to_classify = data_train.copy()\n",
    "# drop shoes, bags and trousers\n",
    "data_hard_to_classify = data_hard_to_classify[data_hard_to_classify[\"label\"] != 1]\n",
    "data_hard_to_classify = data_hard_to_classify[data_hard_to_classify[\"label\"] != 4]\n",
    "data_hard_to_classify = data_hard_to_classify[data_hard_to_classify[\"label\"] != 5]\n",
    "data_hard_to_classify = data_hard_to_classify[data_hard_to_classify[\"label\"] != 7]\n",
    "data_hard_to_classify = data_hard_to_classify[data_hard_to_classify[\"label\"] != 8]\n",
    "data_hard_to_classify = data_hard_to_classify[data_hard_to_classify[\"label\"] != 9]\n",
    "\n",
    "data_easy_to_classify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_easy = np.array(data_easy_to_classify, dtype = 'float32') # Damit Input Daten von Keras akzeptiert werden müssen wir sie in ein Array umwandeln \n",
    "data_train_hard = np.array(data_hard_to_classify, dtype = 'float32') # Damit Input Daten von Keras akzeptiert werden müssen wir sie in ein Array umwandeln \n",
    "data_validate = np.array(data_validate, dtype='float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_easy = data_train_easy[:,1:]  # Added normalization Layer\n",
    "y_train_easy = data_train_easy[:,0] #label data\n",
    "\n",
    "x_train_hard = data_train_hard[:,1:]  # Added normalization Layer\n",
    "y_train_hard = data_train_hard[:,0] #label data\n",
    "\n",
    "#data_submission = data_validate/255  # TODO -0.5?\n",
    "data_submission = data_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the array containing the images (28px x 28px and 1 channel)\n",
    "image_rows = 28\n",
    "image_cols = 28\n",
    "image_shape = (image_rows,image_cols,1)# 1 da schwarz weiß, bei Farbbildern 3 (r,g,b)\n",
    "\n",
    "x_train_easy = x_train_easy.reshape(x_train_easy.shape[0],*image_shape)\n",
    "x_train_hard = x_train_hard.reshape(x_train_hard.shape[0],*image_shape)\n",
    "\n",
    "data_submission = data_submission.reshape(data_submission.shape[0],*image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train data in train and validation set\n",
    "x_train_easy2,x_validate_easy2,y_train_easy2,y_validate_easy2 = train_test_split(x_train_easy,y_train_easy,test_size = 0.2,shuffle=True,random_state = 12345)\n",
    "\n",
    "x_train_hard2,x_validate_hard2,y_train_hard2,y_validate_hard2 = train_test_split(x_train_hard,y_train_hard,test_size = 0.2,shuffle=True,random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x165f623c1c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display tensorflow devices to check for cuda\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import keras\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "# Set GPU as device\n",
    "#tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "tf.config.set_soft_device_placement(False)\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "tf.device(\"/device:GPU:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8Ao3wx_TAzZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,BatchNormalization,SpatialDropout2D,GaussianNoise,Input,Add,Activation,AveragePooling2D,ZeroPadding2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard# zur Visualisierung\n",
    "\n",
    "# Creates layers for data preprocessing -> helps with generalisation\n",
    "# TODO define seed globally\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.Normalization(),\n",
    "        #tf.keras.layers.experimental.preprocessing.RandomRotation(factor=0.25, fill_mode='reflect', interpolation='bilinear', seed=1234, fill_value=0.0),\n",
    "        #tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor=0.2, width_factor=None, fill_mode='reflect', interpolation='bilinear', seed=1234, fill_value=0.0),\n",
    "        #tf.keras.layers.experimental.preprocessing.RandomContrast(factor=0.2, seed=1234),\n",
    "        #tf.keras.layers.experimental.preprocessing.RandomTranslation(height_factor=0.2, width_factor=0.2, fill_mode='reflect', interpolation='bilinear', seed=1234, fill_value=0.0),\n",
    "        #tf.keras.layers.experimental.preprocessing.RandomFlip(mode=\"horizontal_and_vertical\", seed=1234)\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        Conv2D(kernel_size=3,filters=10,activation='relu',input_shape=(28,28,1)),\n",
    "        Flatten(),\n",
    "        #....,\n",
    "        #....,\n",
    "        #....,\n",
    "        #....,\n",
    "        Dense(64,activation = 'relu'),\n",
    "        Dense(10,activation = 'softmax')  # 10 neurons for output, softmax best according to article TODO article here\n",
    "    ], name=\"model\")\n",
    "\n",
    "felix_net = tf.keras.Sequential([\n",
    "    data_augmentation,\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1), padding='same', kernel_regularizer=l2(0.002)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.002)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.002)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.002)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.002)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.002)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "easy_net = tf.keras.Sequential([\n",
    "    data_augmentation,\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "def multi_net_predict(dataset):\n",
    "    prediction = easy_net.predict(dataset)\n",
    "    prediction = np.argmax(prediction,axis = 1)\n",
    "    accuracy = 0\n",
    "    for pred_ind in range(len(prediction)):\n",
    "        if pred_ind == 0:\n",
    "            # Get second opinion\n",
    "            second_op = felix_net.predict(np.array([dataset[pred_ind]]))\n",
    "            second_op = np.argmax(second_op,axis = 1)\n",
    "            \n",
    "            prediction[pred_ind] = second_op[0]\n",
    "        else:\n",
    "            # Transform back to original labels\n",
    "            pass # TODO needed\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def multi_net_evaluate(dataset, labels):\n",
    "    prediction = multi_net_predict(dataset)\n",
    "    \n",
    "    # TODO calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_network = \"\"\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "total_epochs = 50\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FdcLz18_p5z_"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_history = []\n",
    "hard_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "375/375 [==============================] - 53s 90ms/step - loss: 0.6598 - accuracy: 0.7777 - val_loss: 0.2155 - val_accuracy: 0.9181\n",
      "Epoch 2/50\n",
      "344/375 [==========================>...] - ETA: 2s - loss: 0.2318 - accuracy: 0.9120"
     ]
    }
   ],
   "source": [
    "# Train easy network\n",
    "easy_net.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9, decay=0.05/total_epochs),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "easy_history.append(easy_net.fit(\n",
    "    x_train_easy2,\n",
    "    y_train_easy2,\n",
    "    epochs=total_epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    validation_data=(x_validate_easy2,y_validate_easy2),\n",
    "    use_multiprocessing=True,\n",
    "    workers=20,\n",
    "    max_queue_size=50\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train hard network\n",
    "felix_net.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9, decay=0.05/total_epochs),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "hard_history.append(felix_net.fit(\n",
    "    x_train_hard2,\n",
    "    y_train_hard2,\n",
    "    epochs=total_epochs, # use more epochs for alexnet\n",
    "    #epochs=14,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    validation_data=(x_validate_hard2,y_validate_hard2),\n",
    "    use_multiprocessing=True,\n",
    "    workers=20,\n",
    "    max_queue_size=50\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56715,
     "status": "ok",
     "timestamp": 1588762131976,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "xv1ZHQDKVTNf",
    "outputId": "9ab698cd-c2ac-4b17-aeb5-f978cd63ae83"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_validate2,y_validate2,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Easy Network history\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "for his in easy_history:\n",
    "    if his.history.get(\"loss\") is None or his.history.get(\"val_loss\") is None:\n",
    "        continue\n",
    "    train_list.extend(his.history['loss'])\n",
    "    val_list.extend(his.history['val_loss'])\n",
    "\n",
    "plt.plot(train_list, label='Loss')\n",
    "plt.plot(val_list, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Easy Training - Loss Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Easy Network history\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "for his in hard_history:\n",
    "    if his.history.get(\"loss\") is None or his.history.get(\"val_loss\") is None:\n",
    "        continue\n",
    "    train_list.extend(his.history['loss'])\n",
    "    val_list.extend(his.history['val_loss'])\n",
    "\n",
    "plt.plot(train_list, label='Loss')\n",
    "plt.plot(val_list, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Hard Training - Loss Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot easy history\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "for his in easy_history:\n",
    "    if his.history.get(\"accuracy\") is None or his.history.get(\"val_accuracy\") is None:\n",
    "        continue\n",
    "    train_list.extend(his.history['accuracy'])\n",
    "    val_list.extend(his.history['val_accuracy'])\n",
    "\n",
    "plt.plot(train_list, label='Accuracy')\n",
    "plt.plot(val_list, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Easy Training - Accuracy Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot hard training\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "for his in hard_history:\n",
    "    if his.history.get(\"accuracy\") is None or his.history.get(\"val_accuracy\") is None:\n",
    "        continue\n",
    "    train_list.extend(his.history['accuracy'])\n",
    "    val_list.extend(his.history['val_accuracy'])\n",
    "\n",
    "plt.plot(train_list, label='Accuracy')\n",
    "plt.plot(val_list, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Hard Training - Accuracy Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0A6sAXubTIy"
   },
   "source": [
    "### Submission\n",
    "Submit your final notebook as **fashion_mnist_teamX.ipynb** and your predictions of the test data as a **predictions_teamX.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "import sklearn.metrics\n",
    "val_pred = np.argmax(model.predict(x_validate2), axis = 1)\n",
    "\n",
    "conf_matrix = pd.DataFrame(sklearn.metrics.confusion_matrix(y_validate2, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_min_val = pd.read_csv('test_w_labels.csv')\n",
    "data_min_val = np.array(data_min_val, dtype=\"float32\")\n",
    "x_min_val = data_min_val[:,1:]  # Added normalization Layer\n",
    "y_min_val = data_min_val[:,0] #label data\n",
    "\n",
    "x_min_val = x_min_val.reshape(x_min_val.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_pred = model.evaluate(x_min_val, y_min_val, batch_size=128)\n",
    "print('Final Test Loss : {:.4f}'.format(min_val_pred[0]))\n",
    "print('Final Test Accuracy : {:.4f}'.format(min_val_pred[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "val_pred = np.argmax(model.predict(x_min_val), axis = 1)\n",
    "\n",
    "conf_matrix = pd.DataFrame(sklearn.metrics.confusion_matrix(y_min_val, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1588762760277,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "GYlssBbUiqia",
    "outputId": "01aa38ae-2ae2-4a39-e7d7-22aff7ec462e"
   },
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(data_submission)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model to disk\n",
    "# from datetime import datetime\n",
    "# if True:\n",
    "#     model.save(\"trained_models/model_\" + training_network + \"_\"+ datetime.now().strftime(\"%d%m%Y_%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1588762762823,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "INl9cwRWi33g",
    "outputId": "27402ea0-a73a-4fb9-e306-6a776d1cb5ec"
   },
   "outputs": [],
   "source": [
    "# data_results = pd.DataFrame(results)\n",
    "# data_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9RoRppakBZk"
   },
   "outputs": [],
   "source": [
    "# data_results.to_csv('fashion_mnist_pred_team1.csv', index=False)#Bitte statt X eure Gruppennummer einfügen! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Jw6h5WYfeCr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4JfEh7kvx6m"
   },
   "source": [
    "## Ressources\n",
    "Background:\n",
    "  * Book: [Neural Networks and Deep Learning, Michael Nielsen](http://neuralnetworksanddeeplearning.com) \n",
    "  * Lecture: [CS231n, Stanford University](http://cs231n.stanford.edu/)\n",
    "\n",
    "Implementation:\n",
    "  * [TensorFlow tutorials](https://www.tensorflow.org/tutorials)\n",
    "  * [Keras Docs](https://www.tensorflow.org/api_docs/python/tf/keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOkMb2Px8JOS"
   },
   "source": [
    "## Image Sources\n",
    "* http://neuralnetworksanddeeplearning.com/images/\n",
    "* https://www.researchgate.net/publication/320270458/figure/fig1/AS:551197154254848@1508427050805/Mathematical-model-of-artificial-neuron.png\n",
    "* https://www.w3resource.com/w3r_images/numpy-manipulation-ndarray-flatten-function-image-1.png\n",
    "* https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\n",
    "* https://glassboxmedicine.files.wordpress.com/2019/01/slide2.jpg?w=616\n",
    "* http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png\n",
    "* https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "digit_recognition_baseline_final.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb",
     "timestamp": 1587494630831
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
