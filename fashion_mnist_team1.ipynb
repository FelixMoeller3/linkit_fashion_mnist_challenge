{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDb1ilq8WZ5r"
   },
   "source": [
    "# Fashion MNIST Data Science Challenge: Neural Networks and Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pe17JjfE-VYl"
   },
   "source": [
    "This file contains the code as well as the explanations to our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyYtP8bPm8e3",
    "outputId": "cd0f81a3-ec44-4c69-d749-47cddf3d887c"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "FILE_PREFIX = \"\"\n",
    "MODEL_PATH = FILE_PREFIX + \"trained_models/model_{}\".format(datetime.datetime.now().strftime(\"%d%m%Y_%H%M%S\"))\n",
    "WEIGHTS_PATH = MODEL_PATH + \"/best_weights_checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBvQp_U8-VYm"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xTS0YM-sOJyf"
   },
   "outputs": [],
   "source": [
    "#basic packages\n",
    "import typing\n",
    "import math  # needed by keras\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,BatchNormalization,GaussianNoise\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set random state for constant results\n",
    "np.random.RandomState(12345)  \n",
    "tf.random.set_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vkwr2vKM-VYn"
   },
   "outputs": [],
   "source": [
    "# When true explanatory graphs and tensorboard are rendered -> impacts runtime\n",
    "SHOW_EXPLANATORY_GRAPHS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9IAoEDMJmUL1"
   },
   "outputs": [],
   "source": [
    "# Save best weights of the model while training and load afterwards\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=WEIGHTS_PATH,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z33CvoON9uoF"
   },
   "source": [
    "## Data Preparation and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSa-yR1Y-VYp"
   },
   "source": [
    "### Random erasing\n",
    "We are using a third-party github repository that implements random_erasing. You can find it [here](https://github.com/yu4u/cutout-random-erasing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EQeZmOdt-VYp"
   },
   "outputs": [],
   "source": [
    "# Import random eraser from its python file\n",
    "%run random_eraser/random_eraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPorIwMB-VYp"
   },
   "source": [
    "### Image augmentation\n",
    "This function is used for data augmentation. It randomly changes the brightness or contrast of the image, deletes radom portions and possibly flips and rotates the input image. This expands the data set which improves the model's capability of generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LfgqCGFu-VYq"
   },
   "outputs": [],
   "source": [
    "# About 50% Chance that any augmentation happens\n",
    "ROTATION_CHANCE = 0.1\n",
    "FLIP_CHANCE = 0.2 # actually 0.1 since tf.image.random_flip_left_right has 0.5 chance\n",
    "CONSTRAST_CHANCE = 0.2 # actually 0.1 since tf.image.random_contrast has 0.5 chance\n",
    "BRIGHTNESS_CHANCE = 0.2 # actually 0.1 since tf.image.random_brightness has 0.5 chance\n",
    "ERASER_CHANCE = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kM-c92u5-VYr"
   },
   "outputs": [],
   "source": [
    "# function that takes an image as input and creates a randomly augmented image as output\n",
    "def get_dataset_augmentation_func(eraser_chance:float = ERASER_CHANCE,\n",
    "                                  rotation_chance:float = ROTATION_CHANCE,\n",
    "                                  flip_chance:float = FLIP_CHANCE,\n",
    "                                  contrast_chance:float = CONSTRAST_CHANCE,\n",
    "                                  brightness_chance:float = BRIGHTNESS_CHANCE):\n",
    "    def dataset_augmentation(image: tf.Tensor)->np.ndarray:\n",
    "        output_image = image\n",
    "        \n",
    "        ######## ERASER ########\n",
    "        eraser = get_random_eraser(p=eraser_chance, s_l=0.04, s_h=0.2, v_l=-1, v_h=1, pixel_level=True)\n",
    "        if output_image.ndim == 3 or output_image.ndim == 2:\n",
    "            output_image = eraser(output_image)    \n",
    "        else:\n",
    "            for im_indx, image in enumerate(output_image):\n",
    "                output_image[im_indx] = eraser(image)\n",
    "\n",
    "        ######## ROTATE ########\n",
    "        augmentation_chance = np.random.random_sample(size=None)\n",
    "        rotation_amount = np.random.randint(0, high=4)\n",
    "        if (augmentation_chance <= rotation_chance):\n",
    "            # rotation_amount to degrees:\n",
    "            # 0 = 0°\n",
    "            # 1 = 90°\n",
    "            # 2 = 180°\n",
    "            # 3 = 270°\n",
    "            # 4 (not possible) = 360° = 0°\n",
    "            output_image = tf.image.rot90(output_image, k=rotation_amount)\n",
    "\n",
    "        ######## FLIP ########\n",
    "        augmentation_chance = np.random.random_sample(size=None)\n",
    "        if (augmentation_chance <= flip_chance):\n",
    "            output_image = tf.image.random_flip_left_right(output_image, seed=12345)\n",
    "\n",
    "        # ######## CONTRAST ########\n",
    "        # augmentation_chance = np.random.random_sample(size=None)\n",
    "        # if (augmentation_chance <= contrast_chance):\n",
    "        #     output_image = tf.image.random_contrast(output_image, 0.2, 0.5, seed=12345) \n",
    "\n",
    "        # ######## BRIGHTNESS ########\n",
    "        # augmentation_chance = np.random.random_sample(size=None)\n",
    "        # if (augmentation_chance <= brightness_chance):\n",
    "        #     output_image = tf.image.random_brightness(output_image, 0.2, seed=12345) \n",
    "\n",
    "        return output_image\n",
    "    \n",
    "    return dataset_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R05AWEJ3-VYs"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4Fptwh8hO62p"
   },
   "outputs": [],
   "source": [
    "#1. Get the file\n",
    "data_train = pd.read_csv(FILE_PREFIX + 'train.csv')\n",
    "data_validate = pd.read_csv(FILE_PREFIX + 'test.csv')\n",
    "data_train = np.array(data_train, dtype = 'float32') # Damit Input Daten von Keras akzeptiert werden müssen wir sie in ein Array umwandeln \n",
    "data_validate = np.array(data_validate, dtype='float32') \n",
    "\n",
    "# Since we are using a Normalization layer and a Dataset, the preprocessing is only \n",
    "# reshaping the arrays and splitting the sets\n",
    "x_train = data_train[:,1:]\n",
    "y_train = data_train[:,0] #label data\n",
    "\n",
    "data_submission = data_validate\n",
    "\n",
    "#reshape the array containing the images (28px x 28px and 1 channel)\n",
    "image_rows = 28\n",
    "image_cols = 28\n",
    "image_shape = (image_rows,image_cols,1)# 1 da schwarz weiß, bei Farbbildern 3 (r,g,b)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],*image_shape)\n",
    "data_submission = data_submission.reshape(data_submission.shape[0],*image_shape)\n",
    "\n",
    "#split train data in train and validation set\n",
    "x_train2,x_validate2,y_train2,y_validate2 = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzVKM7ow9yyu"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEWLs6ls-VYt"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIOgbVAB-VYt"
   },
   "source": [
    "Mostly we decided our parameters by doing hyperparameter tuning via trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wiRMj8qM-VYu"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 405\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 512\n",
    "L2_PENALTY = 0.003\n",
    "DATA_MEAN = np.mean(x_train2, axis=(0,1,2,3))\n",
    "DATA_VARIANCE = np.var(x_train2, axis=(0,1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3jSkE8a-VYu"
   },
   "source": [
    "#### Optimizer\n",
    "There are multiple articles mentioning SGD as the best optimizer in the long term.\n",
    "[This article](https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/) compares the Adam and SGD optimizer and concludes that SGD with momentum and Nesterov results in a better validation accuracy.\n",
    "Other articles suggest that even though Adam is faster in the beginning but SGD has better convergence in the long run.\n",
    "\n",
    "Even though SGD might take more epochs to train, the network's accuracy ends up to be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ingJqwgf-VYu"
   },
   "outputs": [],
   "source": [
    "our_optimizer = SGD(learning_rate=INIT_LR, momentum=0.9, decay=INIT_LR/max(NUM_EPOCHS, 1))  # max to prevent divide by 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9umJOH9-VYu"
   },
   "source": [
    "#### Loss Function\n",
    "\n",
    "The aim of this chapter is to evaluate the best loss function on an architecture. We do this by taking a subset of the training. Then we train our models with the different loss functions. Note that all loss functions are trained with the same data. After the training we evaluate the accuracy with: 1. training sample and 2. validation sample for our loss functions. To ensure that we are statistically correct we repeat this `n_survey` times. We aggregate by taking the mean of all  `n_survey` as well as the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "avQTuytT-VYu"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SHOW_EXPLANATORY_GRAPHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8829a90d220f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mSHOW_EXPLANATORY_GRAPHS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mn_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mn_survey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mn_models\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SHOW_EXPLANATORY_GRAPHS' is not defined"
     ]
    }
   ],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    n_epoch = 4\n",
    "    n_survey = 6\n",
    "    n_models = 3\n",
    "    cnt = 0\n",
    "    # initialize accuracy and its variance\n",
    "    acc = np.zeros((n_epoch,n_models))\n",
    "    acc_sq = np.zeros((n_epoch,n_models))\n",
    "    # initialize validation accuracy and its variance\n",
    "    vacc = np.zeros((n_epoch,n_models))\n",
    "    vacc_sq = np.zeros((n_epoch,n_models))\n",
    "\n",
    "    for ii in np.random.randint(1,100000,n_survey):\n",
    "        # split rain data in train and validation set, use different random states\n",
    "        x_tr,x_val,y_tr,y_val = train_test_split(x_train2,y_train2,test_size = 0.2,random_state = ii)\n",
    "        # update count\n",
    "        print('__________ ',np.round(cnt/n_survey *100),'% __________')\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "        # iterate through different models\n",
    "        for jj in range(n_models):\n",
    "            if jj==0:\n",
    "                ## sparse categorical crossentropy\n",
    "                # simple baseline model (to beat)\n",
    "                msimple = Sequential([\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu',input_shape=(28,28,1)),\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu'),\n",
    "                    Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "                    Flatten(),\n",
    "                    Dense(256, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(10,activation='softmax')\n",
    "                ])\n",
    "\n",
    "                # compile model\n",
    "                msimple.compile(optimizer='adam',\n",
    "                                loss='sparse_categorical_crossentropy',\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "                # train model\n",
    "                wsimple = msimple.fit(x_tr,y_tr,epochs=n_epoch,verbose=0,\n",
    "                                      batch_size=64,\n",
    "                                      validation_data=(x_val,y_val))\n",
    "\n",
    "                print('\\n sparse categorical crossentropy: ',end='')\n",
    "\n",
    "\n",
    "            elif jj==1:\n",
    "                ## categorical crossentropy\n",
    "                # simple baseline model (to beat)\n",
    "                msimple = Sequential([\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu',input_shape=(28,28,1)),\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu'),\n",
    "                    Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "                    Flatten(),\n",
    "                    Dense(256, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(10,activation='softmax')\n",
    "                ])\n",
    "\n",
    "                # compile model\n",
    "                msimple.compile(optimizer='adam',\n",
    "                                loss='categorical_crossentropy',\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "                # train model\n",
    "                wsimple = msimple.fit(x_tr,to_categorical(y_tr),epochs=n_epoch,verbose=0,\n",
    "                                      batch_size=64,\n",
    "                                      validation_data=(x_val,to_categorical(y_val)))\n",
    "\n",
    "                print('\\n categorical crossentropy: ', end='')\n",
    "\n",
    "\n",
    "            elif jj==2:\n",
    "                ## categorical hinge\n",
    "                # simple baseline model (to beat)\n",
    "                msimple = Sequential([\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu',input_shape=(28,28,1)),\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu'),\n",
    "                    Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "                    Flatten(),\n",
    "                    Dense(256, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(10,activation='softmax')\n",
    "                ])\n",
    "\n",
    "                # compile model\n",
    "                msimple.compile(optimizer='adam',\n",
    "                                loss='categorical_hinge',\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "                # train model\n",
    "                wsimple = msimple.fit(x_tr,to_categorical(y_tr),epochs=n_epoch,verbose=0,\n",
    "                                      batch_size=64,\n",
    "                                      validation_data=(x_val,to_categorical(y_val)))\n",
    "\n",
    "                print('\\n categorical hinge: ',end='')\n",
    "\n",
    "            # save model acc\n",
    "            tmp1 = wsimple.history['accuracy']\n",
    "            acc[:,jj] += np.copy(tmp1) /n_survey\n",
    "            acc_sq[:,jj] += np.copy(tmp1)**2 /n_survey\n",
    "            tmp2 = wsimple.history['val_accuracy']\n",
    "            vacc[:,jj] += np.copy(tmp2) /n_survey\n",
    "            vacc_sq[:,jj] += np.copy(tmp2)**2 /n_survey\n",
    "            print('acc ',np.round(np.max(tmp1),4),', val acc: ',np.round(np.max(tmp2),4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pC3qFjbL-VYw"
   },
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    acc_std_err = (acc_sq - acc**2)**.5\n",
    "    vacc_std_err = (vacc_sq - vacc**2)**.5\n",
    "    models=['sparse categorical crossentropy','categorical crossentropy',\n",
    "           'categorical hinge']\n",
    "    mcolor = ['r','b','y']\n",
    "\n",
    "    fig = plt.figure(figsize=(13,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('training accuracy')\n",
    "    for jj in range(n_models):\n",
    "        plt.plot(np.arange(1,5),acc[:,jj],mcolor[jj]+'-', label=models[jj]);\n",
    "        plt.plot(np.arange(1,5),acc[:,jj]+acc_std_err[:,jj],mcolor[jj]+'o');\n",
    "        plt.plot(np.arange(1,5),acc[:,jj]-acc_std_err[:,jj],mcolor[jj]+'o');\n",
    "        plt.legend(loc='lower right');\n",
    "        plt.xlabel('number of epochs');\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('validation accuracy')\n",
    "    for jj in range(n_models):\n",
    "        plt.plot(np.arange(1,5),vacc[:,jj],mcolor[jj]+'-', label=models[jj]);\n",
    "        plt.plot(np.arange(1,5),vacc[:,jj]+vacc_std_err[:,jj],mcolor[jj]+'o');\n",
    "        plt.plot(np.arange(1,5),vacc[:,jj]-vacc_std_err[:,jj],mcolor[jj]+'o');\n",
    "        plt.legend(loc='lower right');\n",
    "        plt.xlabel('number of epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_FHridc-VYy"
   },
   "source": [
    "Solid lines in the plots indicate the mean over `n_survey` (=6) rounds. The dots are the standard errors. We conclude, that the `categorical_entropy` and `sparse_categorical_crossentropy` are the loss functions we want to consider further. As it turned out, the loss function below it the best for our advanced model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uhN3Zjyz-VYy"
   },
   "outputs": [],
   "source": [
    "our_loss_function = 'sparse_categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjzcYnGr-VYy"
   },
   "source": [
    "#### Activation Functions\n",
    "For the hidden layers we are using ReLU since it is less prone to the vanishing gradient problem. Both ReLU and softmax are standard activation functions for image classification (blog post [here](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "B8Ao3wx_TAzZ"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        # Normalize values from [0,255) to [-1, 1)\n",
    "        Normalization(input_shape=x_train.shape[1:], mean=DATA_MEAN, variance=DATA_VARIANCE),\n",
    "    \n",
    "        #3 convolutional layers followed by a max pooling layer\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        #next 3 convolutional layers followed by max pooling layer\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        GaussianNoise(0.1),\n",
    "        Dropout(0.25),\n",
    "    \n",
    "        #3 dense layers followed by output layer\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a15WW7rj-VYz",
    "outputId": "64c98176-01bb-490a-d924-f147abc1a08d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 28, 28, 1)         3         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 1,050,573\n",
      "Trainable params: 1,049,482\n",
      "Non-trainable params: 1,091\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "u48C9WQ774n4"
   },
   "outputs": [],
   "source": [
    "## Kompilieren des Modells\n",
    "model.compile(optimizer = our_optimizer,\n",
    "                  loss= our_loss_function,\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdcLz18_p5z_"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8iRsZIdH-VY0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_dataset = ImageDataGenerator(horizontal_flip=True, \n",
    "                                      zoom_range=0.1, \n",
    "                                      preprocessing_function=get_dataset_augmentation_func(flip_chance=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ta8C6sWgVS4g",
    "outputId": "1112f9e5-a6e5-4339-ca4b-e766057ea7c1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/405\n",
      "94/94 [==============================] - 27s 255ms/step - loss: 5.6848 - accuracy: 0.2864 - val_loss: 5.8725 - val_accuracy: 0.1987\n",
      "Epoch 2/405\n",
      "94/94 [==============================] - 23s 249ms/step - loss: 4.8914 - accuracy: 0.5430 - val_loss: 5.5993 - val_accuracy: 0.2124\n",
      "Epoch 3/405\n",
      "94/94 [==============================] - 23s 248ms/step - loss: 4.6210 - accuracy: 0.6243 - val_loss: 5.0643 - val_accuracy: 0.4952\n",
      "Epoch 4/405\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 4.4718 - accuracy: 0.6628 - val_loss: 4.4599 - val_accuracy: 0.7117\n",
      "Epoch 5/405\n",
      "94/94 [==============================] - 23s 246ms/step - loss: 4.3509 - accuracy: 0.6909 - val_loss: 4.1332 - val_accuracy: 0.7806\n",
      "Epoch 6/405\n",
      "94/94 [==============================] - 23s 249ms/step - loss: 4.2632 - accuracy: 0.7065 - val_loss: 3.9782 - val_accuracy: 0.8008\n",
      "Epoch 7/405\n",
      "94/94 [==============================] - 23s 246ms/step - loss: 4.1878 - accuracy: 0.7178 - val_loss: 3.8955 - val_accuracy: 0.8156\n",
      "Epoch 8/405\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 4.1150 - accuracy: 0.7276 - val_loss: 3.8398 - val_accuracy: 0.8214\n",
      "Epoch 9/405\n",
      "94/94 [==============================] - 23s 249ms/step - loss: 4.0458 - accuracy: 0.7384 - val_loss: 3.7996 - val_accuracy: 0.8261\n",
      "Epoch 10/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 3.9818 - accuracy: 0.7495 - val_loss: 3.7365 - val_accuracy: 0.8306\n",
      "Epoch 11/405\n",
      "94/94 [==============================] - 23s 250ms/step - loss: 3.9283 - accuracy: 0.7590 - val_loss: 3.6904 - val_accuracy: 0.8382\n",
      "Epoch 12/405\n",
      "94/94 [==============================] - 24s 256ms/step - loss: 3.8666 - accuracy: 0.7673 - val_loss: 3.6327 - val_accuracy: 0.8468\n",
      "Epoch 13/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 3.8160 - accuracy: 0.7746 - val_loss: 3.5912 - val_accuracy: 0.8455\n",
      "Epoch 14/405\n",
      "94/94 [==============================] - 24s 254ms/step - loss: 3.7624 - accuracy: 0.7795 - val_loss: 3.5414 - val_accuracy: 0.8517\n",
      "Epoch 15/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 3.7129 - accuracy: 0.7856 - val_loss: 3.4988 - val_accuracy: 0.8570\n",
      "Epoch 16/405\n",
      "94/94 [==============================] - 24s 253ms/step - loss: 3.6649 - accuracy: 0.7921 - val_loss: 3.4574 - val_accuracy: 0.8591\n",
      "Epoch 17/405\n",
      "94/94 [==============================] - 23s 249ms/step - loss: 3.6164 - accuracy: 0.7975 - val_loss: 3.4142 - val_accuracy: 0.8618\n",
      "Epoch 18/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 3.5689 - accuracy: 0.8024 - val_loss: 3.3702 - val_accuracy: 0.8660\n",
      "Epoch 19/405\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 3.5273 - accuracy: 0.8059 - val_loss: 3.3378 - val_accuracy: 0.8634\n",
      "Epoch 20/405\n",
      "94/94 [==============================] - 24s 253ms/step - loss: 3.4810 - accuracy: 0.8109 - val_loss: 3.2909 - val_accuracy: 0.8700\n",
      "Epoch 21/405\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 3.4414 - accuracy: 0.8129 - val_loss: 3.2548 - val_accuracy: 0.8727\n",
      "Epoch 22/405\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 3.4014 - accuracy: 0.8164 - val_loss: 3.2239 - val_accuracy: 0.8712\n",
      "Epoch 23/405\n",
      "94/94 [==============================] - 23s 250ms/step - loss: 3.3595 - accuracy: 0.8220 - val_loss: 3.1835 - val_accuracy: 0.8753\n",
      "Epoch 24/405\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 3.3196 - accuracy: 0.8246 - val_loss: 3.1418 - val_accuracy: 0.8770\n",
      "Epoch 25/405\n",
      "94/94 [==============================] - 24s 250ms/step - loss: 3.2797 - accuracy: 0.8270 - val_loss: 3.1125 - val_accuracy: 0.8788\n",
      "Epoch 26/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 3.2392 - accuracy: 0.8320 - val_loss: 3.0826 - val_accuracy: 0.8787\n",
      "Epoch 27/405\n",
      "94/94 [==============================] - 24s 250ms/step - loss: 3.2050 - accuracy: 0.8310 - val_loss: 3.0516 - val_accuracy: 0.8779\n",
      "Epoch 28/405\n",
      "94/94 [==============================] - 24s 254ms/step - loss: 3.1601 - accuracy: 0.8377 - val_loss: 3.0161 - val_accuracy: 0.8817\n",
      "Epoch 29/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 3.1302 - accuracy: 0.8377 - val_loss: 2.9709 - val_accuracy: 0.8866\n",
      "Epoch 30/405\n",
      "94/94 [==============================] - 24s 255ms/step - loss: 3.0953 - accuracy: 0.8405 - val_loss: 2.9435 - val_accuracy: 0.8829\n",
      "Epoch 31/405\n",
      "94/94 [==============================] - 24s 250ms/step - loss: 3.0645 - accuracy: 0.8423 - val_loss: 2.9102 - val_accuracy: 0.8874\n",
      "Epoch 32/405\n",
      "94/94 [==============================] - 24s 253ms/step - loss: 3.0291 - accuracy: 0.8444 - val_loss: 2.8908 - val_accuracy: 0.8835\n",
      "Epoch 33/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 2.9949 - accuracy: 0.8449 - val_loss: 2.8495 - val_accuracy: 0.8894\n",
      "Epoch 34/405\n",
      "94/94 [==============================] - 23s 249ms/step - loss: 2.9650 - accuracy: 0.8454 - val_loss: 2.8188 - val_accuracy: 0.8899\n",
      "Epoch 35/405\n",
      "94/94 [==============================] - 24s 253ms/step - loss: 2.9330 - accuracy: 0.8476 - val_loss: 2.7950 - val_accuracy: 0.8898\n",
      "Epoch 36/405\n",
      "94/94 [==============================] - 24s 250ms/step - loss: 2.8979 - accuracy: 0.8500 - val_loss: 2.7694 - val_accuracy: 0.8898\n",
      "Epoch 37/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 2.8662 - accuracy: 0.8511 - val_loss: 2.7267 - val_accuracy: 0.8959\n",
      "Epoch 38/405\n",
      "94/94 [==============================] - 24s 258ms/step - loss: 2.8352 - accuracy: 0.8522 - val_loss: 2.7212 - val_accuracy: 0.8885\n",
      "Epoch 39/405\n",
      "94/94 [==============================] - 24s 254ms/step - loss: 2.8084 - accuracy: 0.8547 - val_loss: 2.6658 - val_accuracy: 0.8985\n",
      "Epoch 40/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 2.7726 - accuracy: 0.8571 - val_loss: 2.6488 - val_accuracy: 0.8949\n",
      "Epoch 41/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 2.7475 - accuracy: 0.8570 - val_loss: 2.6102 - val_accuracy: 0.9000\n",
      "Epoch 42/405\n",
      "94/94 [==============================] - 24s 256ms/step - loss: 2.7213 - accuracy: 0.8568 - val_loss: 2.5835 - val_accuracy: 0.9006\n",
      "Epoch 43/405\n",
      "94/94 [==============================] - 24s 255ms/step - loss: 2.6854 - accuracy: 0.8595 - val_loss: 2.5639 - val_accuracy: 0.8993\n",
      "Epoch 44/405\n",
      "94/94 [==============================] - 23s 249ms/step - loss: 2.6543 - accuracy: 0.8620 - val_loss: 2.5446 - val_accuracy: 0.8966\n",
      "Epoch 45/405\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 2.6315 - accuracy: 0.8615 - val_loss: 2.5167 - val_accuracy: 0.8978\n",
      "Epoch 46/405\n",
      "94/94 [==============================] - 24s 258ms/step - loss: 2.6072 - accuracy: 0.8634 - val_loss: 2.4778 - val_accuracy: 0.9026\n",
      "Epoch 47/405\n",
      "94/94 [==============================] - 24s 255ms/step - loss: 2.5772 - accuracy: 0.8661 - val_loss: 2.4691 - val_accuracy: 0.8986\n",
      "Epoch 48/405\n",
      "94/94 [==============================] - 24s 253ms/step - loss: 2.5509 - accuracy: 0.8661 - val_loss: 2.4260 - val_accuracy: 0.9040\n",
      "Epoch 49/405\n",
      "94/94 [==============================] - 24s 254ms/step - loss: 2.5288 - accuracy: 0.8653 - val_loss: 2.4056 - val_accuracy: 0.9039\n",
      "Epoch 50/405\n",
      "94/94 [==============================] - 24s 255ms/step - loss: 2.4961 - accuracy: 0.8674 - val_loss: 2.3720 - val_accuracy: 0.9073\n",
      "Epoch 51/405\n",
      "94/94 [==============================] - 24s 256ms/step - loss: 2.4722 - accuracy: 0.8679 - val_loss: 2.3545 - val_accuracy: 0.9040\n",
      "Epoch 52/405\n",
      "94/94 [==============================] - 24s 254ms/step - loss: 2.4477 - accuracy: 0.8684 - val_loss: 2.3390 - val_accuracy: 0.9018\n",
      "Epoch 53/405\n",
      "94/94 [==============================] - 24s 254ms/step - loss: 2.4188 - accuracy: 0.8704 - val_loss: 2.3024 - val_accuracy: 0.9061\n",
      "Epoch 54/405\n",
      "94/94 [==============================] - 24s 256ms/step - loss: 2.3906 - accuracy: 0.8741 - val_loss: 2.2791 - val_accuracy: 0.9079\n",
      "Epoch 55/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 2.3653 - accuracy: 0.8756 - val_loss: 2.2598 - val_accuracy: 0.9079\n",
      "Epoch 56/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 2.3453 - accuracy: 0.8726 - val_loss: 2.2648 - val_accuracy: 0.8967\n",
      "Epoch 57/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 2.3222 - accuracy: 0.8750 - val_loss: 2.2098 - val_accuracy: 0.9110\n",
      "Epoch 58/405\n",
      "94/94 [==============================] - 24s 256ms/step - loss: 2.2923 - accuracy: 0.8788 - val_loss: 2.1838 - val_accuracy: 0.9107\n",
      "Epoch 59/405\n",
      "94/94 [==============================] - 24s 254ms/step - loss: 2.2761 - accuracy: 0.8754 - val_loss: 2.1744 - val_accuracy: 0.9072\n",
      "Epoch 60/405\n",
      "94/94 [==============================] - 24s 254ms/step - loss: 2.2483 - accuracy: 0.8775 - val_loss: 2.1424 - val_accuracy: 0.9109\n",
      "Epoch 61/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 2.2288 - accuracy: 0.8791 - val_loss: 2.1236 - val_accuracy: 0.9101\n",
      "Epoch 62/405\n",
      "94/94 [==============================] - 24s 255ms/step - loss: 2.2016 - accuracy: 0.8796 - val_loss: 2.1070 - val_accuracy: 0.9085\n",
      "Epoch 63/405\n",
      "94/94 [==============================] - 24s 255ms/step - loss: 2.1808 - accuracy: 0.8808 - val_loss: 2.0834 - val_accuracy: 0.9105\n",
      "Epoch 64/405\n",
      "94/94 [==============================] - 24s 259ms/step - loss: 2.1572 - accuracy: 0.8815 - val_loss: 2.0701 - val_accuracy: 0.9095\n",
      "Epoch 65/405\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 2.1375 - accuracy: 0.8831 - val_loss: 2.0358 - val_accuracy: 0.9118\n",
      "Epoch 66/405\n",
      "94/94 [==============================] - 24s 255ms/step - loss: 2.1175 - accuracy: 0.8845 - val_loss: 2.0313 - val_accuracy: 0.9077\n",
      "Epoch 67/405\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 2.0984 - accuracy: 0.8830 - val_loss: 1.9928 - val_accuracy: 0.9143\n",
      "Epoch 68/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 2.0759 - accuracy: 0.8838 - val_loss: 1.9817 - val_accuracy: 0.9127\n",
      "Epoch 69/405\n",
      "94/94 [==============================] - 24s 255ms/step - loss: 2.0583 - accuracy: 0.8834 - val_loss: 1.9652 - val_accuracy: 0.9110\n",
      "Epoch 70/405\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 2.0320 - accuracy: 0.8871 - val_loss: 1.9367 - val_accuracy: 0.9146\n",
      "Epoch 71/405\n",
      "94/94 [==============================] - 24s 255ms/step - loss: 2.0169 - accuracy: 0.8858 - val_loss: 1.9234 - val_accuracy: 0.9107\n",
      "Epoch 72/405\n",
      "94/94 [==============================] - 24s 255ms/step - loss: 1.9914 - accuracy: 0.8886 - val_loss: 1.8988 - val_accuracy: 0.9166\n",
      "Epoch 73/405\n",
      "30/94 [========>.....................] - ETA: 16s - loss: 1.9813 - accuracy: 0.8869"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_dataset.flow(x_train2, y_train2, batch_size=BATCH_SIZE),\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_data=(x_validate2, y_validate2),\n",
    "    callbacks=[model_checkpoint_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JlelVOY-VY1"
   },
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "model.load_weights(WEIGHTS_PATH)\n",
    "# Saving the model \n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sTN4OjX-VY1"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZaj6GVO-VY1"
   },
   "source": [
    "We use a custom algorithm for image prediction.\n",
    "### Grid Search\n",
    "In training we used data augmentation in order to generalize. If we learn also augmented data, we also learn augmented structures. What if we augment the validation data, i.e. predict for many different augmented versions of the same image the class probabilities and aggregate afterwards?\n",
    "We kind of steal the idea from the bagging algorithm. With bagging you train many models with different bootstrap samples. Then we predict test data with every model and then aggregate the class probabilities with e.g. the mean.\n",
    "In our case, we just train one model, because training a neural net is very costly. But since we trained with augmentation we want to stabilize the prediction with the mean.\n",
    "\n",
    "### Impact\n",
    "In this implementation we use the same function as for training the model. Thus, our approach is a stochastic augmentation approach. We introduce a new hyperparamter `real_img_weight`. This scales the prediction of the real image, usually with a higher weight than the augmentation grid. The impact at this point is varying with dispersion due to the stochastic implementation. This, however, can be tuned. Due to time restriction, we did not really tune this and due to dispersion we take the prediction of the real images as our final submission. With a little paper search we did not find any paper that introduced augmenting the validation data and aggregation. But we did not really extended this search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJHvFv2C-VY1"
   },
   "outputs": [],
   "source": [
    "def predict(input_data: np.ndarray, grid_size:int = 6) -> np.ndarray:\n",
    "    assert(input_data[0].shape == (28,28,1))\n",
    "    assert(grid_size > 1)\n",
    "    # Init weight of real image\n",
    "    real_img_weight = 0.3\n",
    "\n",
    "    augmentedResults = []\n",
    "    # Predict unaugmented image\n",
    "    augmentedResults.append(model.predict(input_data) * real_img_weight)  \n",
    "    # predict image with augmented data (grid_size - 1 times)\n",
    "    augmentedResults.extend([model.predict(get_dataset_augmentation_func(rotaion_chance=ROTATION_CHANCE*2,\n",
    "                                                                         flip_chance = FLIP_CHANCE*3,\n",
    "                                                                         contrast_chance = CONSTRAST_CHANCE*3,\n",
    "                                                                         brightness_chance = BRIGHTNESS_CHANCE*3\n",
    "                                                                         )(np.copy(input_data)))*(1-w)/(grid_size-1) for _ in range(grid_size - 1)])\n",
    "    \n",
    "    return np.sum(augmentedResults, axis=0)\n",
    "\n",
    "# Returns the accuracy\n",
    "def evaluate(input_data: np.ndarray, input_labels: np.ndarray, grid_size:int = 6) -> float:\n",
    "    assert(len(input_data) == len(input_labels))\n",
    "    prediction = predict(input_data, grid_size)\n",
    "    prediction = np.argmax(prediction, axis = 1)\n",
    "    \n",
    "    return np.count_nonzero(prediction == input_labels) / len(input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Og0i9CCl-VY1"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xv1ZHQDKVTNf"
   },
   "outputs": [],
   "source": [
    "score_non_grid = model.evaluate(x_validate2,y_validate2,verbose=0)\n",
    "score_grid = evaluate(x_validate2,y_validate2)\n",
    "print('Test Accuracy without grid: {:.4f}'.format(score_non_grid[1]))\n",
    "print('Test Accuracy with grid: {:.4f}'.format(score_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PC7MNtHA-pu"
   },
   "outputs": [],
   "source": [
    "# Test grid search prediction\n",
    "n_survey = 10\n",
    "grid_survey = np.empty((n_survey,))\n",
    "for survey_ind in range(n_survey):\n",
    "    grid_survey[survey_ind] = evaluate(x_validate2,y_validate2)\n",
    "\n",
    "plt.hist(grid_survey) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXeb4Eyh-VY2"
   },
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Training Loss\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEFImb8c-VY2"
   },
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Training Accuracy\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training - Accuracy Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acrdrErS-VY2"
   },
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Confusion matrix with grid\n",
    "    val_pred = np.argmax(predict(x_validate2), axis = 1)\n",
    "\n",
    "    conf_matrix = pd.DataFrame(confusion_matrix(y_validate2, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrJPX_3z-VY3"
   },
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Confusion matrix without grid\n",
    "    val_pred = np.argmax(model.predict(x_validate2), axis = 1)\n",
    "\n",
    "    conf_matrix = pd.DataFrame(confusion_matrix(y_validate2, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0A6sAXubTIy"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYlssBbUiqia"
   },
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(data_submission)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "data_results = pd.DataFrame(results)\n",
    "data_results.to_csv(FILE_PREFIX + 'fashion_mnist_pred_team1.csv', index=False)#Bitte statt X eure Gruppennummer einfügen! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ykyw6o-w-VY3"
   },
   "source": [
    "## Our Learnings\n",
    "In the following part we reflect on our network and mention multiple points that could be improved for better classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ5ygSx7-VY4"
   },
   "source": [
    "### Parameters\n",
    "Currently the hyperparameter tuning is done manually, but there are libraries (Like [this one](https://keras-team.github.io/keras-tuner/)) that implement systematic hyperparameter tuning. Since this process takes long and is very expensive (each setting needs to be tested after each other), and we (sadly) do not have the computational power to execute the process, we decided against using it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2ZDZrXd-VY4"
   },
   "source": [
    "### Network Structure\n",
    "In order to keep the computational complexity as low as possible we decided to use a more shallow structure.\n",
    "Even though it performs very well research as well as the [official zalando github](https://github.com/zalandoresearch/fashion-mnist) suggest that a deeper network topography contributes to a more accurate classificaion. One of the inspirations for our network structure ([article here](https://medium.com/@mjbhobe/classifying-fashion-with-a-keras-cnn-achieving-94-accuracy-part-3-c7ca2919232b)) also mentions the benefits of a deeper network.\n",
    "\n",
    "Additionally other structures like DenseNet (described in [this paper](https://arxiv.org/abs/1608.06993)) or ResNet variations can be used to boost accuracy. Tensorflow offers a wide range of pretrained models that can be easily used ([list here](https://www.tensorflow.org/api_docs/python/tf/keras/applications)).\n",
    "\n",
    "Also using cross validation (as described [here](https://towardsdatascience.com/3-methods-to-reduce-overfitting-of-machine-learning-models-4a7e2c1da9ef) and [here](https://www.machinecurve.com/index.php/2020/02/18/how-to-use-k-fold-cross-validation-with-keras/)) can help with minimizing overfitting, but it is computationally rather expensive since multiple models have to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7T0sA3t-VY4"
   },
   "source": [
    "### Data Preprocessing\n",
    "While we used rudimentary data preprocessing functions, the model can benefit from a more extensive use of these methods. [This library](https://github.com/mdbloice/Augmentor) implements various image preprocessing functions that are well-suited for this purpose. Nethertheless the currently implemented preprocessor already increased the model's capability of generalization. A downside is that the accuracy fluctautes heavily in the beginning and training takes more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RET_EBW-VY4"
   },
   "source": [
    "### Errors\n",
    "The confusion matrix shows, that the network displays poor classification capabilities on some classes (like dresses and shirts) whereas it has no difficulty categorizing others (i.e. boots and bags). Sadly we were not able to eliminate these shortcomings. The main problem for this is the lack of training data, which cannot fully be overcome by data augmentation. However, as we have mentioned before, we believe that a higher accuracy is only obtainable for much deeper networks. Another problem we face is that our model still tends to overfit the training data. We have already tackled this issue by using batch normalisation layers, l2 penalty and dropout without complete success. Surprisingly, we found that increasing measures to overcome overfitting decreased the models overall performance which is why we chose not to tackle overfitting more."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "fashion_mnist_team1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
