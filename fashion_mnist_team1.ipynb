{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDb1ilq8WZ5r"
   },
   "source": [
    "# Fashion MNIST Data Science Challenge: Neural Networks and Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pe17JjfE-VYl"
   },
   "source": [
    "This file contains the code as well as the explanations to our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyYtP8bPm8e3",
    "outputId": "cd0f81a3-ec44-4c69-d749-47cddf3d887c"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "FILE_PREFIX = \"\"\n",
    "MODEL_PATH = FILE_PREFIX + \"trained_models/model_{}\".format(datetime.datetime.now().strftime(\"%d%m%Y_%H%M%S\"))\n",
    "WEIGHTS_PATH = MODEL_PATH + \"/best_weights_checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBvQp_U8-VYm"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xTS0YM-sOJyf"
   },
   "outputs": [],
   "source": [
    "#basic packages\n",
    "import typing\n",
    "import math  # needed by keras\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,BatchNormalization,GaussianNoise\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set random state for constant results\n",
    "np.random.RandomState(12345)  \n",
    "tf.random.set_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vkwr2vKM-VYn"
   },
   "outputs": [],
   "source": [
    "# When true explanatory graphs and tensorboard are rendered -> impacts runtime\n",
    "SHOW_EXPLANATORY_GRAPHS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9IAoEDMJmUL1"
   },
   "outputs": [],
   "source": [
    "# Save best weights of the model while training and load afterwards\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=WEIGHTS_PATH,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z33CvoON9uoF"
   },
   "source": [
    "## Data Preparation and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSa-yR1Y-VYp"
   },
   "source": [
    "### Random erasing\n",
    "We are using a third-party github repository that implements random_erasing. You can find it [here](https://github.com/yu4u/cutout-random-erasing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EQeZmOdt-VYp"
   },
   "outputs": [],
   "source": [
    "# Import random eraser from its python file\n",
    "%run random_eraser/random_eraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPorIwMB-VYp"
   },
   "source": [
    "### Image augmentation\n",
    "This function is used for data augmentation. It randomly changes the brightness or contrast of the image, deletes radom portions and possibly flips and rotates the input image. This expands the data set which improves the model's capability of generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LfgqCGFu-VYq"
   },
   "outputs": [],
   "source": [
    "# About 50% Chance that any augmentation happens\n",
    "ROTATION_CHANCE = 0.1\n",
    "FLIP_CHANCE = 0.2 # actually 0.1 since tf.image.random_flip_left_right has 0.5 chance\n",
    "CONSTRAST_CHANCE = 0.2 # actually 0.1 since tf.image.random_contrast has 0.5 chance\n",
    "BRIGHTNESS_CHANCE = 0.2 # actually 0.1 since tf.image.random_brightness has 0.5 chance\n",
    "ERASER_CHANCE = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kM-c92u5-VYr"
   },
   "outputs": [],
   "source": [
    "# function that takes an image as input and creates a randomly augmented image as output\n",
    "def get_dataset_augmentation_func(eraser_chance:float = ERASER_CHANCE,\n",
    "                                  rotation_chance:float = ROTATION_CHANCE,\n",
    "                                  flip_chance:float = FLIP_CHANCE,\n",
    "                                  contrast_chance:float = CONSTRAST_CHANCE,\n",
    "                                  brightness_chance:float = BRIGHTNESS_CHANCE) -> typing.Callable[[np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Creates a function that augments the input image or input image array with the\n",
    "    given probabilities\n",
    "    :param eraser_chance: Probability that a random portion of the image will be erased\n",
    "    :type eraser_chance: float between 0 and 1\n",
    "    :param rotation_chance: Probability that the image will be rotated by a multiple of 90 degrees (also random)\n",
    "    :type rotation_chance: float between 0 and 1\n",
    "    :param flip_chance: Probability that the image will be flipped left and right\n",
    "    :type flip_chance: float between 0 and 1\n",
    "    :param contrast_chance: Probability that the image contrast will be changed (currently not used)\n",
    "    :type contrast_chance: float between 0 and 1\n",
    "    :param brightness_chance: Probability that the image brightness will be changed (currently not used)\n",
    "    :type brightness_chance: float between 0 and 1\n",
    "    :return: A function that takes either an image (numpy array with shape (height, width, channels) or\n",
    "                (height, width)) or a list with multiple images and returns the image(s) with augmentations\n",
    "    :rtype: Callable[[np.ndarray], np.ndarray]\n",
    "    \"\"\"\n",
    "    def dataset_augmentation(image: np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        :param image: image or a list with multiple images that will be augmented\n",
    "        :type image: numpy array with shape (height, width, channels) or (height, width) or (num_images, image.shape)\n",
    "        :return: Augmented image(s)\n",
    "        :rtype: numpy array with shape (height, width, channels) or (height, width) or (num_images, image.shape)\n",
    "        \"\"\"\n",
    "        output_image = image\n",
    "        \n",
    "        ######## ERASER ########\n",
    "        eraser = get_random_eraser(p=eraser_chance, s_l=0.04, s_h=0.2, v_l=-1, v_h=1, pixel_level=True)\n",
    "        if output_image.ndim == 3 or output_image.ndim == 2:\n",
    "            output_image = eraser(output_image)    \n",
    "        else:\n",
    "            for im_indx, image in enumerate(output_image):\n",
    "                output_image[im_indx] = eraser(image)\n",
    "\n",
    "        ######## ROTATE ########\n",
    "        augmentation_chance = np.random.random_sample(size=None)\n",
    "        rotation_amount = np.random.randint(0, high=4)\n",
    "        if (augmentation_chance <= rotation_chance):\n",
    "            # rotation_amount to degrees:\n",
    "            # 0 = 0°\n",
    "            # 1 = 90°\n",
    "            # 2 = 180°\n",
    "            # 3 = 270°\n",
    "            # 4 (not possible) = 360° = 0°\n",
    "            output_image = tf.image.rot90(output_image, k=rotation_amount)\n",
    "\n",
    "        ######## FLIP ########\n",
    "        augmentation_chance = np.random.random_sample(size=None)\n",
    "        if (augmentation_chance <= flip_chance):\n",
    "            output_image = tf.image.random_flip_left_right(output_image, seed=12345)\n",
    "\n",
    "        # ######## CONTRAST ########\n",
    "        # augmentation_chance = np.random.random_sample(size=None)\n",
    "        # if (augmentation_chance <= contrast_chance):\n",
    "        #     output_image = tf.image.random_contrast(output_image, 0.2, 0.5, seed=12345) \n",
    "\n",
    "        # ######## BRIGHTNESS ########\n",
    "        # augmentation_chance = np.random.random_sample(size=None)\n",
    "        # if (augmentation_chance <= brightness_chance):\n",
    "        #     output_image = tf.image.random_brightness(output_image, 0.2, seed=12345) \n",
    "\n",
    "        return output_image\n",
    "    \n",
    "    return dataset_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R05AWEJ3-VYs"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4Fptwh8hO62p"
   },
   "outputs": [],
   "source": [
    "#1. Get the file\n",
    "data_train = pd.read_csv(FILE_PREFIX + 'train.csv')\n",
    "data_validate = pd.read_csv(FILE_PREFIX + 'test.csv')\n",
    "data_train = np.array(data_train, dtype = 'float32') # Damit Input Daten von Keras akzeptiert werden müssen wir sie in ein Array umwandeln \n",
    "data_validate = np.array(data_validate, dtype='float32') \n",
    "\n",
    "# Since we are using a Normalization layer and a Dataset, the preprocessing is only \n",
    "# reshaping the arrays and splitting the sets\n",
    "x_train = data_train[:,1:]\n",
    "y_train = data_train[:,0] #label data\n",
    "\n",
    "data_submission = data_validate\n",
    "\n",
    "#reshape the array containing the images (28px x 28px and 1 channel)\n",
    "image_rows = 28\n",
    "image_cols = 28\n",
    "image_shape = (image_rows,image_cols,1)# 1 da schwarz weiß, bei Farbbildern 3 (r,g,b)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],*image_shape)\n",
    "data_submission = data_submission.reshape(data_submission.shape[0],*image_shape)\n",
    "\n",
    "#split train data in train and validation set\n",
    "x_train2,x_validate2,y_train2,y_validate2 = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzVKM7ow9yyu"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEWLs6ls-VYt"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIOgbVAB-VYt"
   },
   "source": [
    "Mostly we decided our parameters by doing hyperparameter tuning via trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wiRMj8qM-VYu"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 500\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 512\n",
    "L2_PENALTY = 0.003\n",
    "DATA_MEAN = np.mean(x_train2, axis=(0,1,2,3))\n",
    "DATA_VARIANCE = np.var(x_train2, axis=(0,1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3jSkE8a-VYu"
   },
   "source": [
    "#### Optimizer\n",
    "There are multiple articles mentioning SGD as the best optimizer in the long term.\n",
    "[This article](https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/) compares the Adam and SGD optimizer and concludes that SGD with momentum and Nesterov results in a better validation accuracy.\n",
    "Other articles suggest that even though Adam is faster in the beginning but SGD has better convergence in the long run.\n",
    "\n",
    "Even though SGD might take more epochs to train, the network's accuracy ends up to be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ingJqwgf-VYu"
   },
   "outputs": [],
   "source": [
    "our_optimizer = SGD(learning_rate=INIT_LR, momentum=0.9, decay=INIT_LR/max(NUM_EPOCHS, 1))  # max to prevent divide by 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9umJOH9-VYu"
   },
   "source": [
    "#### Loss Function\n",
    "\n",
    "The aim of this chapter is to evaluate the best loss function on an architecture. We do this by taking a subset of the training. Then we train our models with the different loss functions. Note that all loss functions are trained with the same data. After the training we evaluate the accuracy with: 1. training sample and 2. validation sample for our loss functions. To ensure that we are statistically correct we repeat this `n_survey` times. We aggregate by taking the mean of all  `n_survey` as well as the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "avQTuytT-VYu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________  0.0 % __________\n",
      "\n",
      " sparse categorical crossentropy: acc  0.8936 , val acc:  0.8906\n",
      "\n",
      " categorical crossentropy: acc  0.888 , val acc:  0.8873\n",
      "\n",
      " categorical hinge: acc  0.107 , val acc:  0.0934\n",
      "__________  17.0 % __________\n",
      "\n",
      " sparse categorical crossentropy: acc  0.8911 , val acc:  0.8843\n",
      "\n",
      " categorical crossentropy: acc  0.8901 , val acc:  0.8863\n",
      "\n",
      " categorical hinge: acc  0.1484 , val acc:  0.1031\n",
      "__________  33.0 % __________\n",
      "\n",
      " sparse categorical crossentropy: acc  0.8898 , val acc:  0.8859\n",
      "\n",
      " categorical crossentropy: acc  0.8914 , val acc:  0.8883\n",
      "\n",
      " categorical hinge: acc  0.1131 , val acc:  0.097\n",
      "__________  50.0 % __________\n",
      "\n",
      " sparse categorical crossentropy: acc  0.8894 , val acc:  0.8935\n",
      "\n",
      " categorical crossentropy: acc  0.892 , val acc:  0.8967\n",
      "\n",
      " categorical hinge: acc  0.8027 , val acc:  0.8197\n",
      "__________  67.0 % __________\n",
      "\n",
      " sparse categorical crossentropy: acc  0.8919 , val acc:  0.8954\n",
      "\n",
      " categorical crossentropy: acc  0.8849 , val acc:  0.8921\n",
      "\n",
      " categorical hinge: acc  0.7985 , val acc:  0.824\n",
      "__________  83.0 % __________\n",
      "\n",
      " sparse categorical crossentropy: acc  0.8914 , val acc:  0.8961\n",
      "\n",
      " categorical crossentropy: acc  0.8892 , val acc:  0.8873\n",
      "\n",
      " categorical hinge: acc  0.17 , val acc:  0.1023\n"
     ]
    }
   ],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    n_epoch = 4\n",
    "    n_survey = 6\n",
    "    n_models = 3\n",
    "    cnt = 0\n",
    "    # initialize accuracy and its variance\n",
    "    acc = np.zeros((n_epoch,n_models))\n",
    "    acc_sq = np.zeros((n_epoch,n_models))\n",
    "    # initialize validation accuracy and its variance\n",
    "    vacc = np.zeros((n_epoch,n_models))\n",
    "    vacc_sq = np.zeros((n_epoch,n_models))\n",
    "\n",
    "    for ii in np.random.randint(1,100000,n_survey):\n",
    "        # split rain data in train and validation set, use different random states\n",
    "        x_tr,x_val,y_tr,y_val = train_test_split(x_train2,y_train2,test_size = 0.2,random_state = ii)\n",
    "        # update count\n",
    "        print('__________ ',np.round(cnt/n_survey *100),'% __________')\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "        # iterate through different models\n",
    "        for jj in range(n_models):\n",
    "            if jj==0:\n",
    "                ## sparse categorical crossentropy\n",
    "                # simple baseline model (to beat)\n",
    "                msimple = Sequential([\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu',input_shape=(28,28,1)),\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu'),\n",
    "                    Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "                    Flatten(),\n",
    "                    Dense(256, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(10,activation='softmax')\n",
    "                ])\n",
    "\n",
    "                # compile model\n",
    "                msimple.compile(optimizer='adam',\n",
    "                                loss='sparse_categorical_crossentropy',\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "                # train model\n",
    "                wsimple = msimple.fit(x_tr,y_tr,epochs=n_epoch,verbose=0,\n",
    "                                      batch_size=64,\n",
    "                                      validation_data=(x_val,y_val))\n",
    "\n",
    "                print('\\n sparse categorical crossentropy: ',end='')\n",
    "\n",
    "\n",
    "            elif jj==1:\n",
    "                ## categorical crossentropy\n",
    "                # simple baseline model (to beat)\n",
    "                msimple = Sequential([\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu',input_shape=(28,28,1)),\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu'),\n",
    "                    Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "                    Flatten(),\n",
    "                    Dense(256, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(10,activation='softmax')\n",
    "                ])\n",
    "\n",
    "                # compile model\n",
    "                msimple.compile(optimizer='adam',\n",
    "                                loss='categorical_crossentropy',\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "                # train model\n",
    "                wsimple = msimple.fit(x_tr,to_categorical(y_tr),epochs=n_epoch,verbose=0,\n",
    "                                      batch_size=64,\n",
    "                                      validation_data=(x_val,to_categorical(y_val)))\n",
    "\n",
    "                print('\\n categorical crossentropy: ', end='')\n",
    "\n",
    "\n",
    "            elif jj==2:\n",
    "                ## categorical hinge\n",
    "                # simple baseline model (to beat)\n",
    "                msimple = Sequential([\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu',input_shape=(28,28,1)),\n",
    "                    Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu'),\n",
    "                    Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "                    Flatten(),\n",
    "                    Dense(256, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dropout(.3),\n",
    "                    Dense(10,activation='softmax')\n",
    "                ])\n",
    "\n",
    "                # compile model\n",
    "                msimple.compile(optimizer='adam',\n",
    "                                loss='categorical_hinge',\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "                # train model\n",
    "                wsimple = msimple.fit(x_tr,to_categorical(y_tr),epochs=n_epoch,verbose=0,\n",
    "                                      batch_size=64,\n",
    "                                      validation_data=(x_val,to_categorical(y_val)))\n",
    "\n",
    "                print('\\n categorical hinge: ',end='')\n",
    "\n",
    "            # save model acc\n",
    "            tmp1 = wsimple.history['accuracy']\n",
    "            acc[:,jj] += np.copy(tmp1) /n_survey\n",
    "            acc_sq[:,jj] += np.copy(tmp1)**2 /n_survey\n",
    "            tmp2 = wsimple.history['val_accuracy']\n",
    "            vacc[:,jj] += np.copy(tmp2) /n_survey\n",
    "            vacc_sq[:,jj] += np.copy(tmp2)**2 /n_survey\n",
    "            print('acc ',np.round(np.max(tmp1),4),', val acc: ',np.round(np.max(tmp2),4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pC3qFjbL-VYw"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    acc_std_err = (acc_sq - acc**2)**.5\n",
    "    vacc_std_err = (vacc_sq - vacc**2)**.5\n",
    "    models=['sparse categorical crossentropy','categorical crossentropy',\n",
    "           'categorical hinge']\n",
    "    mcolor = ['r','b','y']\n",
    "\n",
    "    fig = plt.figure(figsize=(13,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('training accuracy')\n",
    "    for jj in range(n_models):\n",
    "        plt.plot(np.arange(1,5),acc[:,jj],mcolor[jj]+'-', label=models[jj]);\n",
    "        plt.plot(np.arange(1,5),acc[:,jj]+acc_std_err[:,jj],mcolor[jj]+'o');\n",
    "        plt.plot(np.arange(1,5),acc[:,jj]-acc_std_err[:,jj],mcolor[jj]+'o');\n",
    "        plt.legend(loc='lower right');\n",
    "        plt.xlabel('number of epochs');\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('validation accuracy')\n",
    "    for jj in range(n_models):\n",
    "        plt.plot(np.arange(1,5),vacc[:,jj],mcolor[jj]+'-', label=models[jj]);\n",
    "        plt.plot(np.arange(1,5),vacc[:,jj]+vacc_std_err[:,jj],mcolor[jj]+'o');\n",
    "        plt.plot(np.arange(1,5),vacc[:,jj]-vacc_std_err[:,jj],mcolor[jj]+'o');\n",
    "        plt.legend(loc='lower right');\n",
    "        plt.xlabel('number of epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_FHridc-VYy"
   },
   "source": [
    "Solid lines in the plots indicate the mean over `n_survey` (=6) rounds. The dots are the standard errors. We conclude, that the `categorical_entropy` and `sparse_categorical_crossentropy` are the loss functions we want to consider further. As it turned out, the loss function below it the best for our advanced model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uhN3Zjyz-VYy"
   },
   "outputs": [],
   "source": [
    "our_loss_function = 'sparse_categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjzcYnGr-VYy"
   },
   "source": [
    "#### Activation Functions\n",
    "For the hidden layers we are using ReLU since it is less prone to the vanishing gradient problem. Both ReLU and softmax are standard activation functions for image classification (blog post [here](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "B8Ao3wx_TAzZ"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        # Normalize values from [0,255) to [-1, 1)\n",
    "        Normalization(input_shape=x_train.shape[1:], mean=DATA_MEAN, variance=DATA_VARIANCE),\n",
    "    \n",
    "        #3 convolutional layers followed by a max pooling layer\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        #next 3 convolutional layers followed by max pooling layer\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        GaussianNoise(0.1),\n",
    "        Dropout(0.25),\n",
    "    \n",
    "        #3 dense layers followed by output layer\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a15WW7rj-VYz",
    "outputId": "64c98176-01bb-490a-d924-f147abc1a08d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 28, 28, 1)         3         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 1,050,573\n",
      "Trainable params: 1,049,482\n",
      "Non-trainable params: 1,091\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "u48C9WQ774n4"
   },
   "outputs": [],
   "source": [
    "## Kompilieren des Modells\n",
    "model.compile(optimizer = our_optimizer,\n",
    "                  loss= our_loss_function,\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdcLz18_p5z_"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8iRsZIdH-VY0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_dataset = ImageDataGenerator(horizontal_flip=True, \n",
    "                                      zoom_range=0.1, \n",
    "                                      preprocessing_function=get_dataset_augmentation_func(flip_chance=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ta8C6sWgVS4g",
    "outputId": "1112f9e5-a6e5-4339-ca4b-e766057ea7c1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "94/94 [==============================] - 75s 705ms/step - loss: 6.0415 - accuracy: 0.1823 - val_loss: 5.9054 - val_accuracy: 0.1383\n",
      "Epoch 2/500\n",
      "94/94 [==============================] - 60s 634ms/step - loss: 4.9908 - accuracy: 0.5230 - val_loss: 5.7720 - val_accuracy: 0.1075\n",
      "Epoch 3/500\n",
      "94/94 [==============================] - 60s 641ms/step - loss: 4.6701 - accuracy: 0.6223 - val_loss: 5.3271 - val_accuracy: 0.3594\n",
      "Epoch 4/500\n",
      "69/94 [=====================>........] - ETA: 15s - loss: 4.5084 - accuracy: 0.6583"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-47700f476b81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtraining_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_validate2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validate2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\verena\\anaconda3\\envs\\challengeenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\verena\\anaconda3\\envs\\challengeenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\verena\\anaconda3\\envs\\challengeenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\verena\\anaconda3\\envs\\challengeenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\verena\\anaconda3\\envs\\challengeenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32md:\\verena\\anaconda3\\envs\\challengeenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\verena\\anaconda3\\envs\\challengeenv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_dataset.flow(x_train2, y_train2, batch_size=BATCH_SIZE),\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_data=(x_validate2, y_validate2),\n",
    "    callbacks=[model_checkpoint_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JlelVOY-VY1"
   },
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "model.load_weights(WEIGHTS_PATH)\n",
    "# Saving the model \n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sTN4OjX-VY1"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZaj6GVO-VY1"
   },
   "source": [
    "We use a custom algorithm for image prediction.\n",
    "### Grid Search\n",
    "In training we used data augmentation in order to generalize. If we learn also augmented data, we also learn augmented structures. What if we augment the validation data, i.e. predict for many different augmented versions of the same image the class probabilities and aggregate afterwards?\n",
    "We kind of steal the idea from the bagging algorithm. With bagging you train many models with different bootstrap samples. Then we predict test data with every model and then aggregate the class probabilities with e.g. the mean.\n",
    "In our case, we just train one model, because training a neural net is very costly. But since we trained with augmentation we want to stabilize the prediction with the mean.\n",
    "\n",
    "### Impact\n",
    "In this implementation we use the same function as for training the model. Thus, our approach is a stochastic augmentation approach. We introduce a new hyperparamter `real_img_weight`. This scales the prediction of the real image, usually with a higher weight than the augmentation grid. The impact at this point is varying with dispersion due to the stochastic implementation. This, however, can be tuned. Due to time restriction, we did not really tune this and due to dispersion we take the prediction of the real images as our final submission. With a little paper search we did not find any paper that introduced augmenting the validation data and aggregation. But we did not really extended this search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJHvFv2C-VY1"
   },
   "outputs": [],
   "source": [
    "def predict(input_data: np.ndarray, grid_size:int = 6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Custom predict function using grid search\n",
    "    :param input_data: Input images that will be predicted\n",
    "    :type input_data: An array containing images of shape(28, 28, 1)\n",
    "    :param grid_size: Number of tests performed\n",
    "    :type grid_size: int > 2\n",
    "    :return: The predicted classes\n",
    "    :rtype: numpy array with shape (len(input_data), 10)\n",
    "    \"\"\"\n",
    "    assert(input_data[0].shape == (28,28,1))\n",
    "    assert(grid_size > 1)\n",
    "    # Init weight of real image\n",
    "    real_img_weight = 0.3\n",
    "\n",
    "    augmentedResults = []\n",
    "    # Predict unaugmented image\n",
    "    augmentedResults.append(model.predict(input_data) * real_img_weight)  \n",
    "    # predict image with augmented data (grid_size - 1 times)\n",
    "    augmentedResults.extend([model.predict(get_dataset_augmentation_func(rotaion_chance=ROTATION_CHANCE*2,\n",
    "                                                                         flip_chance = FLIP_CHANCE*3,\n",
    "                                                                         contrast_chance = CONSTRAST_CHANCE*3,\n",
    "                                                                         brightness_chance = BRIGHTNESS_CHANCE*3\n",
    "                                                                         )(np.copy(input_data)))*(1-w)/(grid_size-1) for _ in range(grid_size - 1)])\n",
    "    \n",
    "    return np.sum(augmentedResults, axis=0)\n",
    "\n",
    "# Returns the accuracy\n",
    "def evaluate(input_data: np.ndarray, input_labels: np.ndarray, grid_size:int = 6) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the given input images\n",
    "    :param input_data: Input images that will be predicted\n",
    "    :type input_data: An array containing images of shape(28, 28, 1)\n",
    "    :param input_labels: Labels matching the images\n",
    "    :type input_labels: A float array of the shape (len(input_images))\n",
    "    :param grid_size: Number of tests performed\n",
    "    :type grid_size: int > 2\n",
    "    :return: The accuracy of the model\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    assert(len(input_data) == len(input_labels))\n",
    "    prediction = predict(input_data, grid_size)\n",
    "    prediction = np.argmax(prediction, axis = 1)\n",
    "    \n",
    "    return np.count_nonzero(prediction == input_labels) / len(input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Og0i9CCl-VY1"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xv1ZHQDKVTNf"
   },
   "outputs": [],
   "source": [
    "score_non_grid = model.evaluate(x_validate2,y_validate2,verbose=0)\n",
    "score_grid = evaluate(x_validate2,y_validate2)\n",
    "print('Test Accuracy without grid: {:.4f}'.format(score_non_grid[1]))\n",
    "print('Test Accuracy with grid: {:.4f}'.format(score_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PC7MNtHA-pu"
   },
   "outputs": [],
   "source": [
    "# Test grid search prediction\n",
    "n_survey = 10\n",
    "grid_survey = np.empty((n_survey,))\n",
    "for survey_ind in range(n_survey):\n",
    "    grid_survey[survey_ind] = evaluate(x_validate2,y_validate2)\n",
    "\n",
    "plt.hist(grid_survey) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXeb4Eyh-VY2"
   },
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Training Loss\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEFImb8c-VY2"
   },
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Training Accuracy\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training - Accuracy Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acrdrErS-VY2"
   },
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Confusion matrix with grid\n",
    "    val_pred = np.argmax(predict(x_validate2), axis = 1)\n",
    "\n",
    "    conf_matrix = pd.DataFrame(confusion_matrix(y_validate2, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrJPX_3z-VY3"
   },
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Confusion matrix without grid\n",
    "    val_pred = np.argmax(model.predict(x_validate2), axis = 1)\n",
    "\n",
    "    conf_matrix = pd.DataFrame(confusion_matrix(y_validate2, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0A6sAXubTIy"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYlssBbUiqia"
   },
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(data_submission)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "data_results = pd.DataFrame(results)\n",
    "data_results.to_csv(FILE_PREFIX + 'fashion_mnist_pred_team1.csv', index=False)#Bitte statt X eure Gruppennummer einfügen! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ykyw6o-w-VY3"
   },
   "source": [
    "## Our Learnings\n",
    "In the following part we reflect on our network and mention multiple points that could be improved for better classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ5ygSx7-VY4"
   },
   "source": [
    "### Parameters\n",
    "Currently the hyperparameter tuning is done manually, but there are libraries (Like [this one](https://keras-team.github.io/keras-tuner/)) that implement systematic hyperparameter tuning. Since this process takes long and is very expensive (each setting needs to be tested after each other), and we (sadly) do not have the computational power to execute the process, we decided against using it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2ZDZrXd-VY4"
   },
   "source": [
    "### Network Structure\n",
    "In order to keep the computational complexity as low as possible we decided to use a more shallow structure.\n",
    "Even though it performs very well research as well as the [official zalando github](https://github.com/zalandoresearch/fashion-mnist) suggest that a deeper network topography contributes to a more accurate classificaion. One of the inspirations for our network structure ([article here](https://medium.com/@mjbhobe/classifying-fashion-with-a-keras-cnn-achieving-94-accuracy-part-3-c7ca2919232b)) also mentions the benefits of a deeper network.\n",
    "\n",
    "Additionally other structures like DenseNet (described in [this paper](https://arxiv.org/abs/1608.06993)) or ResNet variations can be used to boost accuracy. Tensorflow offers a wide range of pretrained models that can be easily used ([list here](https://www.tensorflow.org/api_docs/python/tf/keras/applications)).\n",
    "\n",
    "Also using cross validation (as described [here](https://towardsdatascience.com/3-methods-to-reduce-overfitting-of-machine-learning-models-4a7e2c1da9ef) and [here](https://www.machinecurve.com/index.php/2020/02/18/how-to-use-k-fold-cross-validation-with-keras/)) can help with minimizing overfitting, but it is computationally rather expensive since multiple models have to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7T0sA3t-VY4"
   },
   "source": [
    "### Data Preprocessing\n",
    "While we used rudimentary data preprocessing functions, the model can benefit from a more extensive use of these methods. [This library](https://github.com/mdbloice/Augmentor) implements various image preprocessing functions that are well-suited for this purpose. Nethertheless the currently implemented preprocessor already increased the model's capability of generalization. A downside is that the accuracy fluctautes heavily in the beginning and training takes more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RET_EBW-VY4"
   },
   "source": [
    "### Errors\n",
    "The confusion matrix shows, that the network displays poor classification capabilities on some classes (like dresses and shirts) whereas it has no difficulty categorizing others (i.e. boots and bags). Sadly we were not able to eliminate these shortcomings. The main problem for this is the lack of training data, which cannot fully be overcome by data augmentation. However, as we have mentioned before, we believe that a higher accuracy is only obtainable for much deeper networks. Another problem we face is that our model still tends to overfit the training data. We have already tackled this issue by using batch normalisation layers, l2 penalty and dropout without complete success. Surprisingly, we found that increasing measures to overcome overfitting decreased the models overall performance which is why we chose not to tackle overfitting more."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "fashion_mnist_team1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
