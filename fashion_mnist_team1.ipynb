{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDb1ilq8WZ5r"
   },
   "source": [
    "# Fashion MNIST Data Science Challenge: Neural Networks and Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code as well as the explanations to our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4071,
     "status": "ok",
     "timestamp": 1588762079160,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "xTS0YM-sOJyf",
    "outputId": "5e739605-4550-4146-a90e-bd809119b88e"
   },
   "outputs": [],
   "source": [
    "#basic packages\n",
    "import typing\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,BatchNormalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.RandomState(12345)  # Set random state for constant results\n",
    "tf.random.set_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When true explanatory graphs and tensorboard are rendered -> impacts runtime\n",
    "SHOW_EXPLANATORY_GRAPHS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up data set cache\n",
    "CACHE_DIR = \"cache\"\n",
    "VALIDATION_CACHE_FILE = os.path.join(CACHE_DIR, \"validation_data_cache.tfcache\")\n",
    "TRAINING_CACHE_FILE = os.path.join(CACHE_DIR, \"training_data_cache.tfcache\")\n",
    "\n",
    "if os.path.isdir(CACHE_DIR):\n",
    "    # delete dir\n",
    "    shutil.rmtree(CACHE_DIR)\n",
    "\n",
    "os.mkdir(CACHE_DIR)\n",
    "\n",
    "if os.path.isfile(VALIDATION_CACHE_FILE):\n",
    "    os.remove(VALIDATION_CACHE_FILE)\n",
    "if os.path.isfile(TRAINING_CACHE_FILE):\n",
    "    os.remove(TRAINING_CACHE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z33CvoON9uoF"
   },
   "source": [
    "## Data Preparation and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <span style=\"color:red\">*TODO BEWEISE + Methoden*</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random erasing\n",
    "We are using a third-party github repository that implements random_erasing. You can find it [here](https://github.com/yu4u/cutout-random-erasing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import random eraser from its python file\n",
    "%run random_eraser/random_eraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image rotation and flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image rotation was used for data augmentation. The function names are self-explanatory\n",
    "def flip_image_vertical(image: np.ndarray) -> np.ndarray:      \n",
    "    return np.flip(image, axis=1)\n",
    "\n",
    "def flip_image_horizontal(image: np.ndarray) -> np.ndarray:\n",
    "    return np.flip(image, axis=0)\n",
    "\n",
    "def rotate_270(image: np.ndarray) -> np.ndarray:\n",
    "    return np.rot90(image, -1)\n",
    "\n",
    "def rotate_90(image: np.ndarray) -> np.ndarray:\n",
    "    return np.rot90(image, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images(input_data: np.ndarray)->typing.Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    flipped_images_vertical = np.empty(shape=input_data.shape)\n",
    "    flipped_images_horizontal = np.empty(shape=input_data.shape)\n",
    "    rot_images_90 = np.empty(shape=input_data.shape)\n",
    "    rot_images_270 = np.empty(shape=input_data.shape)\n",
    "    \n",
    "    for ind, im in enumerate(input_data):\n",
    "        flipped_images_vertical[ind] = flip_image_vertical(im)\n",
    "        flipped_images_horizontal[ind] = flip_image_horizontal(im)\n",
    "        rot_images_90[ind] = rotate_90(im)\n",
    "        rot_images_270[ind] = rotate_270(im)\n",
    "        \n",
    "    return flipped_images_vertical, flipped_images_horizontal, rot_images_90, rot_images_270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 50% Chance that any augmentation happens\n",
    "ROTATION_CHANCE = 0.1\n",
    "FLIP_CHANCE = 0.2 # actually 0.1 since tf.image.random_flip_left_right has 0.5 chance\n",
    "CONSTRAST_CHANCE = 0.2 # actually 0.1 since tf.image.random_contrast has 0.5 chance\n",
    "BRIGHTNESS_CHANCE = 0.2 # actually 0.1 since tf.image.random_brightness has 0.5 chance\n",
    "ERASER_CHANCE = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes an image as input and creates a randomly augmented image as output\n",
    "def dataset_aumentation(image: tf.Tensor, label)->np.ndarray:\n",
    "\n",
    "    output_image = image\n",
    "    \n",
    "    ######## ROTATE ########\n",
    "    augmentation_chance = np.random.random_sample(size=None)\n",
    "    rotation_amount = np.random.randint(0, high=4)\n",
    "    if (augmentation_chance <= ROTATION_CHANCE):\n",
    "        # rotation_amount to degrees:\n",
    "        # 0 = 0°\n",
    "        # 1 = 90°\n",
    "        # 2 = 180°\n",
    "        # 3 = 270°\n",
    "        # 4 (not possible) = 360° = 0°\n",
    "        output_image = tf.image.rot90(output_image, k=rotation_amount)\n",
    "    \n",
    "    ######## FLIP ########\n",
    "    augmentation_chance = np.random.random_sample(size=None)\n",
    "    if (augmentation_chance <= FLIP_CHANCE):\n",
    "        output_image = tf.image.random_flip_left_right(output_image, seed=12345)\n",
    "        \n",
    "    ######## CONTRAST ########\n",
    "    augmentation_chance = np.random.random_sample(size=None)\n",
    "    if (augmentation_chance <= CONSTRAST_CHANCE):\n",
    "        output_image = tf.image.random_contrast(output_image, 0.2, 0.5, seed=12345) \n",
    "        \n",
    "    ######## BRIGHTNESS ########\n",
    "    augmentation_chance = np.random.random_sample(size=None)\n",
    "    if (augmentation_chance <= BRIGHTNESS_CHANCE):\n",
    "        output_image = tf.image.random_brightness(output_image, 0.2, seed=12345) \n",
    "        \n",
    "    ######## ERASER ########\n",
    "    # Uses ndim so convert to numpy array\n",
    "    # Does not work yet\n",
    "\n",
    "#     if image is not None:\n",
    "#         conv_tensor = image.eval(session=tf.compat.v1.Session())\n",
    "#         output_image =  get_random_eraser(p=ERASER_CHANCE, s_l=0.04, s_h=0.2, v_l=-1, v_h=1, pixel_level=True)(conv_tensor)\n",
    "        \n",
    "    return output_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Fptwh8hO62p"
   },
   "outputs": [],
   "source": [
    "#1. Get the file\n",
    "data_train = pd.read_csv('train.csv')\n",
    "data_validate = pd.read_csv('test.csv')\n",
    "data_train = np.array(data_train, dtype = 'float32') # Damit Input Daten von Keras akzeptiert werden müssen wir sie in ein Array umwandeln \n",
    "data_validate = np.array(data_validate, dtype='float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are using a Normalization layer and a Dataset, the preprocessing is only \n",
    "# reshaping the arrays and splitting the sets\n",
    "x_train = data_train[:,1:]\n",
    "y_train = data_train[:,0] #label data\n",
    "\n",
    "data_submission = data_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the array containing the images (28px x 28px and 1 channel)\n",
    "image_rows = 28\n",
    "image_cols = 28\n",
    "image_shape = (image_rows,image_cols,1)# 1 da schwarz weiß, bei Farbbildern 3 (r,g,b)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],*image_shape)\n",
    "data_submission = data_submission.reshape(data_submission.shape[0],*image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train data in train and validation set\n",
    "x_train2,x_validate2,y_train2,y_validate2 = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT_AMOUNT = 2  # How many times the data set should be repeated in one epoch\n",
    "# random eraser cannot be used inside Dataset since the tensor needs to be transformed to a numpy array which does not \n",
    "# work inside Dataset.map since the code is not executed eagerly inside\n",
    "# Also evaluating the tensor does not work since map gives a Placeholder instead of the values\n",
    "# also make_ndarry does not work for unknown reasons (bug in tensorflow or sth)\n",
    "# so the data has to prepared manually\n",
    "# since the data is prepared manually the repeating is done here instead of at Dataset.repeat\n",
    "\n",
    "x_train2 = np.concatenate([x_train2 for _ in range(REPEAT_AMOUNT)])\n",
    "y_train2 = np.concatenate([y_train2 for _ in range(REPEAT_AMOUNT)])\n",
    "\n",
    "eraser = get_random_eraser(p=ERASER_CHANCE, s_l=0.04, s_h=0.2, v_l=-1, v_h=1, pixel_level=True)\n",
    "x_train2 = np.array([eraser(im) for im in x_train2])\n",
    "\n",
    "x_validate2 = np.concatenate([x_validate2 for _ in range(REPEAT_AMOUNT)])\n",
    "y_validate2 = np.concatenate([y_validate2 for _ in range(REPEAT_AMOUNT)])\n",
    "\n",
    "eraser = get_random_eraser(p=ERASER_CHANCE, s_l=0.04, s_h=0.2, v_l=-1, v_h=1, pixel_level=True)\n",
    "x_validate2 = np.array([eraser(im) for im in x_validate2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzVKM7ow9yyu"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly we decided our parameters by doing hyperparameter tuning via trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@felix dein code hier\n",
    "NUM_EPOCHS = 200\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "L2_PENALTY = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "There are multiple articles mentioning SGD as the best optimizer in the long term.\n",
    "[This article](https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/) compares the Adam and SGD optimizer and concludes that SGD with momentum and Nesterov results in a better validation accuracy.\n",
    "Other articles suggest that even though Adam is faster in the beginning but SGD has better convergence in the long run.\n",
    "\n",
    "Even though SGD might take more epochs to train, the network's accuracy ends up to be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_optimizer = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR/max(NUM_EPOCHS, 1))  # max to prevent divide by 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "  <span style=\"color:red\">*Explanation*</span>.\n",
    "  The aim of this chapter is to evaluate the best loss function on an architecture. We do this by taking a subset of the training. Then we train our models with the different loss functions. Note that all loss functions are trained with the same data. After the training we evaluate the accuracy with: 1. training sample and 2. validation sample for our loss functions. To ensure that we are statistically correct we repeat this `n_survey` times. We aggregate by taking the mean of all  `n_survey` as well as the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for dropouts\n",
    "tf.random.set_seed(53124)\n",
    "\n",
    "# set params and seed to reproduce code\n",
    "np.random.seed(12498)\n",
    "n_epoch = 4\n",
    "n_survey = 6\n",
    "n_models = 3\n",
    "cnt = 0\n",
    "# initialize accuracy and its variance\n",
    "acc = np.zeros((n_epoch,n_models))\n",
    "acc_sq = np.zeros((n_epoch,n_models))\n",
    "# initialize validation accuracy and its variance\n",
    "vacc = np.zeros((n_epoch,n_models))\n",
    "vacc_sq = np.zeros((n_epoch,n_models))\n",
    "\n",
    "for ii in np.random.randint(1,100000,n_survey):\n",
    "    # split rain data in train and validation set, use different random states\n",
    "    x_tr,x_val,y_tr,y_val = train_test_split(x_train2,y_train2,test_size = 0.2,random_state = ii)\n",
    "    # update count\n",
    "    print('__________ ',np.round(cnt/n_survey *100),'% __________')\n",
    "    cnt += 1\n",
    "    \n",
    "    \n",
    "    # iterate through different models\n",
    "    for jj in range(n_models):\n",
    "        if jj==0:\n",
    "            ## sparse categorical crossentropy\n",
    "            # simple baseline model (to beat)\n",
    "            msimple = Sequential([\n",
    "                Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu',input_shape=(28,28,1)),\n",
    "                Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu'),\n",
    "                Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(256, activation='relu'),\n",
    "                Dropout(.3),\n",
    "                Dense(128, activation='relu'),\n",
    "                Dropout(.3),\n",
    "                Dense(10,activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            # compile model\n",
    "            msimple.compile(optimizer='adam',\n",
    "                            loss='sparse_categorical_crossentropy',\n",
    "                            metrics=['accuracy'])\n",
    "            \n",
    "            # train model\n",
    "            wsimple = msimple.fit(x_tr,y_tr,epochs=n_epoch,verbose=0,\n",
    "                                  batch_size=64,\n",
    "                                  validation_data=(x_validate,y_validate))\n",
    "            \n",
    "            print('\\n sparse categorical crossentropy: ',end='')\n",
    "            \n",
    "            \n",
    "        elif jj==1:\n",
    "            ## categorical crossentropy\n",
    "            # simple baseline model (to beat)\n",
    "            msimple = Sequential([\n",
    "                Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu',input_shape=(28,28,1)),\n",
    "                Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu'),\n",
    "                Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(256, activation='relu'),\n",
    "                Dropout(.3),\n",
    "                Dense(128, activation='relu'),\n",
    "                Dropout(.3),\n",
    "                Dense(10,activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            # compile model\n",
    "            msimple.compile(optimizer='adam',\n",
    "                            loss='categorical_crossentropy',\n",
    "                            metrics=['accuracy'])\n",
    "            \n",
    "            # train model\n",
    "            wsimple = msimple.fit(x_tr,to_categorical(y_tr),epochs=n_epoch,verbose=0,\n",
    "                                  batch_size=64,\n",
    "                                  validation_data=(x_validate2,to_categorical(y_validate2)))\n",
    "            \n",
    "            print('\\n categorical crossentropy: ', end='')\n",
    "            \n",
    "            \n",
    "        elif jj==2:\n",
    "            ## categorical hinge\n",
    "            # simple baseline model (to beat)\n",
    "            msimple = Sequential([\n",
    "                Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu',input_shape=(28,28,1)),\n",
    "                Conv2D(64,(3,3),strides=(2,2),padding='same',activation='relu'),\n",
    "                Conv2D(32,(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(256, activation='relu'),\n",
    "                Dropout(.3),\n",
    "                Dense(128, activation='relu'),\n",
    "                Dropout(.3),\n",
    "                Dense(10,activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            # compile model\n",
    "            msimple.compile(optimizer='adam',\n",
    "                            loss='categorical_hinge',\n",
    "                            metrics=['accuracy'])\n",
    "            \n",
    "            # train model\n",
    "            wsimple = msimple.fit(x_tr,to_categorical(y_tr),epochs=n_epoch,verbose=0,\n",
    "                                  batch_size=64,\n",
    "                                  validation_data=(x_validate2,to_categorical(y_validate2)))\n",
    "            \n",
    "            print('\\n categorical hinge: ',end='')\n",
    "            \n",
    "        # save model acc\n",
    "        tmp1 = wsimple.history['accuracy']\n",
    "        acc[:,jj] += np.copy(tmp1) /n_survey\n",
    "        acc_sq[:,jj] += np.copy(tmp1)**2 /n_survey\n",
    "        tmp2 = wsimple.history['val_accuracy']\n",
    "        vacc[:,jj] += np.copy(tmp2) /n_survey\n",
    "        vacc_sq[:,jj] += np.copy(tmp2)**2 /n_survey\n",
    "        print('acc ',np.round(np.max(tmp1),4),', val acc: ',np.round(np.max(tmp2),4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_std_err = (acc_sq - acc**2)**.5\n",
    "vacc_std_err = (vacc_sq - vacc**2)**.5\n",
    "models=['sparse categorical crossentropy','categorical crossentropy',\n",
    "       'categorical hinge']\n",
    "mcolor = ['r','b','y']\n",
    "\n",
    "fig = plt.figure(figsize=(13,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('training accuracy')\n",
    "for jj in range(n_models):\n",
    "    plt.plot(np.arange(1,5),acc[:,jj],mcolor[jj]+'-', label=models[jj]);\n",
    "    plt.plot(np.arange(1,5),acc[:,jj]+acc_std_err[:,jj],mcolor[jj]+'o');\n",
    "    plt.plot(np.arange(1,5),acc[:,jj]-acc_std_err[:,jj],mcolor[jj]+'o');\n",
    "    plt.legend(loc='lower right');\n",
    "    plt.xlabel('number of epochs');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('validation accuracy')\n",
    "for jj in range(n_models):\n",
    "    plt.plot(np.arange(1,5),vacc[:,jj],mcolor[jj]+'-', label=models[jj]);\n",
    "    plt.plot(np.arange(1,5),vacc[:,jj]+vacc_std_err[:,jj],mcolor[jj]+'o');\n",
    "    plt.plot(np.arange(1,5),vacc[:,jj]-vacc_std_err[:,jj],mcolor[jj]+'o');\n",
    "    plt.legend(loc='lower right');\n",
    "    plt.xlabel('number of epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solid lines in the plots indicate the mean over `n_survey` (=6) rounds. The dots are the standard errors. We conclude, that the `categorical_entropy` and `sparse_categorical_crossentropy` are the loss functions we want to consider further. As it turned out, the loss function below it the best for our advanced model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions\n",
    "  <span style=\"color:red\">*Explanation*</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8Ao3wx_TAzZ"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        # Normalize values from [0,255) to [-1, 1)\n",
    "        Normalization(input_shape=x_train.shape[1:]),\n",
    "    \n",
    "        #3 convolutional layers followed by a max pooling layer\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        #next 3 convolutional layers followed by max pooling layer\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "    \n",
    "        #3 dense layers followed by output layer\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 28, 28, 1)         3         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 1,050,573\n",
      "Trainable params: 1,049,482\n",
      "Non-trainable params: 1,091\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u48C9WQ774n4"
   },
   "source": [
    "## Kompilieren des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = our_optimizer,\n",
    "                  loss= our_loss_function,\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FdcLz18_p5z_"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create data set using guide here https://medium.com/swlh/dump-keras-imagedatagenerator-start-using-tensorflow-tf-data-part-1-a30330bdbca9\n",
    "# TODO double up training set and have over all 50% probability of augmentation happening at all?\n",
    "# Set up training dataset\n",
    "training_dataset = Dataset.from_tensor_slices((x_train2, y_train2))\n",
    "training_dataset = training_dataset.map(dataset_aumentation, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "training_dataset = training_dataset.cache(TRAINING_CACHE_FILE)\n",
    "training_dataset = training_dataset.shuffle(buffer_size=100)  # TODO set buffer size\n",
    "training_dataset = training_dataset.repeat(1)  # TODO number\n",
    "training_dataset = training_dataset.batch(BATCH_SIZE)\n",
    "training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Set up validation dataset\n",
    "validation_dataset = Dataset.from_tensor_slices((x_validate2, y_validate2))\n",
    "validation_dataset = validation_dataset.map(dataset_aumentation, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache(VALIDATION_CACHE_FILE)\n",
    "validation_dataset = validation_dataset.shuffle(buffer_size=100)  # TODO set buffer size\n",
    "validation_dataset = validation_dataset.repeat(1)  # TODO number\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ta8C6sWgVS4g",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "750/750 [==============================] - 70s 87ms/step - loss: 4.8900 - accuracy: 0.5441 - val_loss: 3.7967 - val_accuracy: 0.8371\n",
      "Epoch 2/200\n",
      "750/750 [==============================] - 59s 79ms/step - loss: 3.7887 - accuracy: 0.8210 - val_loss: 3.4288 - val_accuracy: 0.8731\n",
      "Epoch 3/200\n",
      "410/750 [===============>..............] - ETA: 25s - loss: 3.4458 - accuracy: 0.8560"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    #steps_per_epoch=len(x_train2)/ BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    validation_data=validation_dataset,\n",
    "    #validation_steps=len(x_validate2)/ BATCH_SIZE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a custom algorithm for image prediction.\n",
    "### Grid Search\n",
    "When predicting, the image is augmented in multiple ways before forwaring it to the model. Each of these augmented images is evaluated by the model. Afterwards the mean of the resulting categories is calculated and used as actual prediction.\n",
    "\n",
    "### Impact\n",
    "This method results in a more stable prediction.  <span style=\"color:red\">*TODO BEWEISE + Mehr schreiben*</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data: np.ndarray) -> np.ndarray:\n",
    "    assert(input_data[0].shape == (28,28,1))\n",
    "    augmentedResults = []\n",
    "    \n",
    "    augmentedResults.append(model.predict(input_data))\n",
    "    \n",
    "    flipped_images_vertical, flipped_images_horizontal, rot_images_90, rot_images_270 = augment_images(input_data)     \n",
    "    augmentedResults.append(model.predict(flipped_images_vertical))\n",
    "    augmentedResults.append(model.predict(flipped_images_horizontal))\n",
    "    augmentedResults.append(model.predict(rot_images_90))\n",
    "    augmentedResults.append(model.predict(rot_images_270))\n",
    "    \n",
    "    return (np.sum(augmentedResults, axis=0) / len(augmentedResults))\n",
    "\n",
    "# Returns the accuracy\n",
    "def evaluate(input_data: np.ndarray, input_labels: np.ndarray) -> float:\n",
    "    assert(len(input_data) == len(input_labels))\n",
    "    prediction = predict(input_data)\n",
    "    prediction = np.argmax(prediction, axis = 1)\n",
    "    \n",
    "    return np.count_nonzero(prediction == input_labels) / len(input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56715,
     "status": "ok",
     "timestamp": 1588762131976,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "xv1ZHQDKVTNf",
    "outputId": "9ab698cd-c2ac-4b17-aeb5-f978cd63ae83"
   },
   "outputs": [],
   "source": [
    "score_non_grid = model.evaluate(x_validate2,y_validate2,verbose=0)\n",
    "score_grid = evaluate(x_validate2,y_validate2)\n",
    "print('Test Accuracy without grid: {:.4f}'.format(score_non_grid[1]))\n",
    "print('Test Accuracy with grid: {:.4f}'.format(score_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Training Loss\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Training Accuracy\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training - Accuracy Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Confusion matrix with grid\n",
    "    val_pred = np.argmax(predict(x_validate2), axis = 1)\n",
    "\n",
    "    conf_matrix = pd.DataFrame(confusion_matrix(y_validate2, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_EXPLANATORY_GRAPHS:\n",
    "    # Confusion matrix without grid\n",
    "    val_pred = np.argmax(model.predict(x_validate2), axis = 1)\n",
    "\n",
    "    conf_matrix = pd.DataFrame(confusion_matrix(y_validate2, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0A6sAXubTIy"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1588762760277,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "GYlssBbUiqia",
    "outputId": "01aa38ae-2ae2-4a39-e7d7-22aff7ec462e"
   },
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(data_submission)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "data_results = pd.DataFrame(results)\n",
    "data_results.to_csv('fashion_mnist_pred_team1.csv', index=False)#Bitte statt X eure Gruppennummer einfügen! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Learnings\n",
    "In the following part we reflect on our network and mention multiple points that could be improved for better classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Currently the hyperparameter tuning is done manually, but there are libraries (Like [this one](https://keras-team.github.io/keras-tuner/)) that implement systematic hyperparameter tuning. Since this process takes long and is very expensive (each setting needs to be tested after each other), and we (sadly) do not have the computational power to execute the process, we decided against using it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Structure\n",
    "In order to keep the computational complexity as low as possible we decided to use a more shallow structure.\n",
    "Even though it performs very well research as well as the [official zalando github](https://github.com/zalandoresearch/fashion-mnist) suggest that a deeper network topography contributes to a more accurate classificaion.  [TODO](https://medium.com/@mjbhobe/classifying-fashion-with-a-keras-cnn-achieving-94-accuracy-part-3-c7ca2919232b)\n",
    "\n",
    "Additionally other structures like DenseNet (described in [this paper](https://arxiv.org/abs/1608.06993)) or ResNet variations can be used to boost accuracy. Tensorflow offers a wide range of pretrained models that can be easily used ([list here](https://www.tensorflow.org/api_docs/python/tf/keras/applications)).\n",
    "\n",
    "Also using cross validation (as described [here](https://towardsdatascience.com/3-methods-to-reduce-overfitting-of-machine-learning-models-4a7e2c1da9ef) and [here](https://www.machinecurve.com/index.php/2020/02/18/how-to-use-k-fold-cross-validation-with-keras/)) can help with minimizing overfitting, but it is computationally rather expensive since multiple models have to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "While we used rudimentary data preprocessing functions, the model can benefit from a more extensive use of these methods. [This library](https://github.com/mdbloice/Augmentor) implements various image preprocessing functions that are well-suited for this purpose. Nethertheless the currently implemented preprocessor already increased the model's capability of generalization. A downside is that the accuracy fluctautes heavily in the beginning and training takes more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "<span style=\"color:red\">*TODO Text überarbeiten (overfitting ist Problem,wie gegen overfitting vorgehen?, 94% gut, mehr Daten würden helfen)*</span>\n",
    "The confusion matrix shows, that the network displays very poor classification capabilities on some classes (like dresses and shirts) whereas it has no difficulty categorizing others (i.e. boots and bags). Sadly we were not able to eliminate these shortcomings. However one of the methods presented above might be to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workarounds\n",
    "Almost every code uses workarounds and this one is no exception. For example the workaround for using the random eraser could be exchanged with some effort (i.e. converting the random eraser to use a Tensor). This could result in a better runtime performance (since image generation is triggered by tensorflow and potentially calculated on the GPU) as well as the utilization of cache for less memory usage."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "digit_recognition_baseline_final.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb",
     "timestamp": 1587494630831
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
