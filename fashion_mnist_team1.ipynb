{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDb1ilq8WZ5r"
   },
   "source": [
    "# Fashion MNIST Data Science Challenge: Neural Networks and Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code as well as the explanations to our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4071,
     "status": "ok",
     "timestamp": 1588762079160,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "xTS0YM-sOJyf",
    "outputId": "5e739605-4550-4146-a90e-bd809119b88e"
   },
   "outputs": [],
   "source": [
    "#basic packages\n",
    "import typing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,BatchNormalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z33CvoON9uoF"
   },
   "source": [
    "## Data Preparation and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <span style=\"color:red\">*TODO BEWEISE + Methoden*</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image rotation was used for data augmentation. The function names are self-explanatory\n",
    "def flip_image_vertical(image: np.ndarray) -> np.ndarray:\n",
    "    flipped_image = []\n",
    "    for row in image:\n",
    "        flipped_image.append(row[::-1])\n",
    "    return flipped_image\n",
    "\n",
    "def flip_image_horizontal(image: np.ndarray) -> np.ndarray:\n",
    "    flipped_image = np.transpose(image)\n",
    "    return np.transpose(flip_image_vertical(flipped_image))\n",
    "\n",
    "def rotate_270(image: np.ndarray) -> np.ndarray:\n",
    "    return np.transpose(flip_image_vertical(image))\n",
    "\n",
    "def rotate_90(image: np.ndarray) -> np.ndarray:\n",
    "    return flip_image_vertical(np.transpose(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random erasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a function to randomly erase parts of an image (used only for training data).\n",
    "#See docs here: https://github.com/yu4u/cutout-random-erasing\n",
    "def get_random_eraser(p:float=0.5, s_l:float=0.02, s_h:float=0.4, r_1:float=0.3, r_2:float=1/0.3, \n",
    "                      v_l:int=0, v_h:int=255, pixel_level:bool=False)->typing.Callable[[np.ndarray], np.ndarray]:\n",
    "    def eraser(input_img:np.ndarray)->np.ndarray:\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Fptwh8hO62p"
   },
   "outputs": [],
   "source": [
    "#1. Get the file\n",
    "data_train = pd.read_csv('train.csv')\n",
    "data_validate = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.array(data_train, dtype = 'float32') # Damit Input Daten von Keras akzeptiert werden müssen wir sie in ein Array umwandeln \n",
    "data_validate = np.array(data_validate, dtype='float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO temporary for normalization layer usage\n",
    "use_norm_layer = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train[:,1:]\n",
    "\n",
    "if not use_norm_layer:\n",
    "    x_train = data_train[:,1:]/255 #pixel data from 0-1\n",
    "\n",
    "y_train = data_train[:,0] #label data\n",
    "\n",
    "data_submission = data_validate\n",
    "\n",
    "if not use_norm_layer:\n",
    "    data_submission = data_validate/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the array containing the images (28px x 28px and 1 channel)\n",
    "image_rows = 28\n",
    "image_cols = 28\n",
    "image_shape = (image_rows,image_cols,1)# 1 da schwarz weiß, bei Farbbildern 3 (r,g,b)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],*image_shape)\n",
    "data_submission = data_submission.reshape(data_submission.shape[0],*image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train data in train and validation set\n",
    "x_train2,x_validate2,y_train2,y_validate2 = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Felix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##@Verena: Wir müssen uns noch absprechen, wie wir hier sinnvoll den Trainingsdatensatz bearbeiten (d.h. wann normalisieren wir usw.)\n",
    "##Verena: Ich würde zum normalisieren den layer von keras benutzen \n",
    "## -> habe temporär oben einen boolean eingeführt um das ein und aus zu schalten\n",
    "def prepare_training_set(x_train: np.ndarray, y_train: np.ndarray, num_erases:int)->typing.Tuple[np.ndarray, np.ndarray]:\n",
    "    training_set_size= len(x_train)\n",
    "    augmented_images = []\n",
    "    labels = []\n",
    "    for i in range(0,training_set_size):\n",
    "        for j in range(0,num_erases):\n",
    "            duplicated_image = np.array(x_train[i])\n",
    "            augmented_images.append(eraser(duplicated_image))\n",
    "            labels.append(y_train[i])\n",
    "        augmented_images.append(flip_image_horizontal(x_train[i]))\n",
    "        labels.append(y_train[i])\n",
    "        augmented_images.append(rotate_90(x_train[i]))\n",
    "        labels.append(y_train[i])\n",
    "        augmented_images.append(rotate_270(x_train[i]))\n",
    "        labels.append(y_train[i])\n",
    "\n",
    "    np.append(x_train, augmented_images)\n",
    "    np.append(y_train, labels)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tensor board (using https://neptune.ai/blog/tensorboard-tutorial   \n",
    "# and \n",
    "# https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb)\n",
    "\n",
    "# Comment for increased fitting speed\n",
    "from tensorflow.keras.callbacks import TensorBoard# zur Visualisierung\n",
    "import datetime\n",
    "import itertools \n",
    "import io\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "log_parent_folder = \"logs/\" + current_time\n",
    "#log_folder = log_parent_folder + \"/fit/\"\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"]\n",
    "#plot_dir = log_parent_folder + \"/plots/\"\n",
    "#cm_dir = log_parent_folder + \"/cm/\"\n",
    "file_writer_cm = tf.summary.create_file_writer(log_parent_folder)\n",
    "file_writer = tf.summary.create_file_writer(log_parent_folder)\n",
    "\n",
    "def plot_to_image(figure: plt.Figure):    \n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "\n",
    "    digit = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    digit = tf.expand_dims(digit, 0)\n",
    "\n",
    "    return digit\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names: np.ndarray) ->plt.Figure: \n",
    "    figure = plt.figure(figsize=(8, 8)) \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Accent) \n",
    "    plt.title(\"Confusion matrix\") \n",
    "    plt.colorbar() \n",
    "    tick_marks = np.arange(len(class_names)) \n",
    "    plt.xticks(tick_marks, class_names, rotation=45) \n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)  \n",
    "    threshold = cm.max() / 2. \n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):   \n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"   \n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)  \n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.ylabel('True label') \n",
    "    plt.xlabel('Predicted label') \n",
    "\n",
    "    return figure\n",
    "\n",
    "def log_confusion_matrix(epoch: int, logs: typing.Dict[str, float])->None:\n",
    "    predictions = model.predict(x_validate2)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_validate2, predictions)\n",
    "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
    "    cm_image = plot_to_image(figure)\n",
    "    \n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n",
    "\n",
    "        \n",
    "tensorboard_callbacks = [TensorBoard(log_dir=log_parent_folder,\n",
    "                         histogram_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_images=True,\n",
    "                         update_freq='epoch',\n",
    "                         profile_batch=2,\n",
    "                         embeddings_freq=1),\n",
    "                         tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzVKM7ow9yyu"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly we decided our parameters by doing hyperparameter tuning via trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@felix dein code hier\n",
    "NUM_EPOCHS = 200\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "L2_PENALTY = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "There are multiple articles mentioning SGD as the best optimizer in the long term.\n",
    "[This article](https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/) compares the Adam and SGD optimizer and concludes that SGD with momentum and Nesterov results in a better validation accuracy.\n",
    "Other articles suggest that even though Adam is faster in the beginning but SGD has better convergence in the long run.\n",
    "\n",
    "Even though SGD might take more epochs to train, the network's accuracy ends up to be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_optimizer = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR/NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "  <span style=\"color:red\">*Explanation*</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions\n",
    "  <span style=\"color:red\">*Explanation*</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8Ao3wx_TAzZ"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        #3 convolutional layers followed by a max pooling layer\n",
    "        Conv2D(32, kernel_size=3, activation='relu', input_shape=x_train.shape[1:], padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        #next 3 convolutional layers followed by max pooling layer\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "    \n",
    "        #3 dense layers followed by output layer\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        BatchNormalization(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(L2_PENALTY)),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "if use_norm_layer:\n",
    "    model = Sequential([\n",
    "        Normalization(input_shape=x_train.shape[1:]),  # TODO values\n",
    "        model\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_1 (Normalizati (None, 28, 28, 1)         3         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 10)                1050570   \n",
      "=================================================================\n",
      "Total params: 1,050,573\n",
      "Trainable params: 1,049,482\n",
      "Non-trainable params: 1,091\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u48C9WQ774n4"
   },
   "source": [
    "## Kompilieren des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = our_optimizer,\n",
    "                  loss= our_loss_function,\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FdcLz18_p5z_"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ta8C6sWgVS4g",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1005/1500 [===================>..........] - ETA: 17s - loss: 4.7616 - accuracy: 0.5905"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train2,\n",
    "    y_train2,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    validation_data=(x_validate2,y_validate2),\n",
    "    callbacks=tensorboard_callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir={log_parent_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a custom algorithm for image prediction.\n",
    "### Grid Search\n",
    "When predicting, the image is augmented in multiple ways before forwaring it to the model. Each of these augmented images is evaluated by the model. Afterwards the mean of the resulting categories is calculated and used as actual prediction.\n",
    "\n",
    "### Impact\n",
    "This method results in a more stable prediction.  <span style=\"color:red\">*TODO BEWEISE + Mehr schreiben*</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data: np.ndarray) -> np.ndarray:\n",
    "    assert(input_data[0].shape == (28,28,1))\n",
    "    augmentedResults = []\n",
    "    \n",
    "    augmentedResults.append(model.predict(input_data))\n",
    "    augmentedResults.append(model.predict(flip_image_vertical(input_data)))\n",
    "    augmentedResults.append(model.predict(flip_image_horizontal(input_data)))\n",
    "    augmentedResults.append(model.predict(rotate_270(input_data)))\n",
    "    augmentedResults.append(model.predict(rotate_90(input_data)))\n",
    "    \n",
    "    # TODO check axis\n",
    "    return (np.sum(augmentedResults, axis=0) / len(augmentedResults))\n",
    "\n",
    "# Returns the accuracy\n",
    "def evaluate(input_data: np.ndarray, input_labels: np.ndarray) -> float:\n",
    "    assert(len(input_data) == len(input_labels))\n",
    "    prediction = predict(input_data)\n",
    "    prediction = np.argmax(prediction, axis = 1)\n",
    "    \n",
    "    # TODO check calculation\n",
    "    return np.count_nonzero(prediction == input_labels) / len(input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56715,
     "status": "ok",
     "timestamp": 1588762131976,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "xv1ZHQDKVTNf",
    "outputId": "9ab698cd-c2ac-4b17-aeb5-f978cd63ae83"
   },
   "outputs": [],
   "source": [
    "score_non_grid = model.evaluate(x_validate2,y_validate2,verbose=0)\n",
    "score_grid = evaluate(x_validate2,y_validate2)\n",
    "print('Test Accuracy without grid: {:.4f}'.format(score_non_grid[1]))\n",
    "print('Test Accuracy with grid: {:.4f}'.format(score_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loss\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training - Accuracy Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix with grid\n",
    "val_pred = np.argmax(predict(x_validate2), axis = 1)\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(y_validate2, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix without grid\n",
    "val_pred = np.argmax(model.predict(x_validate2), axis = 1)\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(y_validate2, val_pred), index=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0A6sAXubTIy"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1588762760277,
     "user": {
      "displayName": "Philipp Luchner",
      "photoUrl": "",
      "userId": "06440293557215660939"
     },
     "user_tz": -120
    },
    "id": "GYlssBbUiqia",
    "outputId": "01aa38ae-2ae2-4a39-e7d7-22aff7ec462e"
   },
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(data_submission)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "data_results = pd.DataFrame(results)\n",
    "data_results.to_csv('fashion_mnist_pred_team1.csv', index=False)#Bitte statt X eure Gruppennummer einfügen! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Learnings\n",
    "In the following part we reflect on our network and mention multiple points that could be improved for better classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Currently the hyperparameter tuning is done manually, but there are libraries (Like [this one](https://keras-team.github.io/keras-tuner/)) that implement systematic hyperparameter tuning. Since this process takes long and is very expensive (each setting needs to be tested after each other), and we (sadly) do not have the computational power to execute the process, we decided against using it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Structure\n",
    "In order to keep the computational complexity as low as possible we decided to use a more shallow structure.\n",
    "Even though it performs very well research as well as the [official zalando github](https://github.com/zalandoresearch/fashion-mnist) suggest that a deeper network topography contributes to a more accurate classificaion.  <span style=\"color:red\">*TODO ARTIKEL*</span>.\n",
    "\n",
    "Additionally other structures like DenseNet (described in [this paper](https://arxiv.org/abs/1608.06993)) or ResNet variations can be used to boost accuracy. Tensorflow offers a wide range of pretrained models that can be easily used ([list here](https://www.tensorflow.org/api_docs/python/tf/keras/applications))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "While we used rudimentary data preprocessing functions, the model can benefit from a more extensive use of these methods. [This library](https://github.com/mdbloice/Augmentor) implements various image preprocessing functions that are well-suited for this purpose. Nethertheless the currently implemented preprocessor already increased the model's capability of generalization. A downside is that the accuracy fluctautes heavily in the beginning and training takes more epochs.  <span style=\"color:red\">*TODO BEWEISE*</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "<span style=\"color:red\">*TODO name + text*</span>\n",
    "The confusion matrix shows, that the network displays very poor classification capabilities on some classes (like dresses and shirts) whereas it has no difficulty categorizing others (i.e. boots and bags). Sadly we were not able to eliminate these shortcomings. However one of the methods presented above might be to do so."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "digit_recognition_baseline_final.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb",
     "timestamp": 1587494630831
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
